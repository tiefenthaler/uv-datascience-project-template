{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"UV Data Science Project Template","text":"<p>Welcome to the documentation for UV Data Science Project Template. This project demonstrates how to set up a data science environment using Docker, UV, FastAPI, along with other tools for developing python projects.</p> <p>Image by David T. [Source: Astral]</p> <p> </p> <p>Template Project for Developing Data Science Projects with UV. A new project using this template can be created with cookiecutter.</p> <p>This guide provides instructions on how to develop and productionize machine learning applications in a robust and efficient way. It is demonstrated how to achieve this using a modern setup of tools, like UV, Docker, Ruff, PyTest, MkDocs, CI, and more (see Overview Tools Section). The focus of this project is to give an introduction to using those tools and not on how to properly set up a machine learning application (for production). Therefore only a simple machine learning pipeline based on PyTorch/Lightning and FastAPI is used.</p>"},{"location":"#overview-tools","title":"Overview Tools","text":"<p>The project includes the following components, for more details see Documentation - Guides:</p> Tool Description UV A fast and efficient package manager for Python, written in Rust. It replaces tools like pip and virtualenv. Ruff An extremely fast Python linter, formatter, and code assistant, written in Rust. PyRight A static type checker for Python, helping to catch type-related errors early in the development process. PyTest A powerful and flexible testing framework for Python, simplifying writing and running tests. Coverage A tool for measuring code coverage of Python programs, helping to ensure that all parts of the code are tested. Pre-Commit A framework for managing and maintaining multi-language pre-commit hooks to ensure code quality. CI-GitHub Continuous Integration setup using GitHub Actions to automate testing, linting, and deployment. MkDocs A static site generator geared towards building project documentation, written in Markdown. VSCode-DevContainer A development environment setup using Docker and VS Code, providing a consistent and isolated workspace. Docker-Production Docker setup for creating a lean, efficient, and secure production environment for applications. Cookiecutter A command-line utility that creates projects from project templates."},{"location":"#using-uv-to-manage-the-project","title":"Using uv to Manage the Project","text":"<p><code>UV</code> is a tool that simplifies the management of Python projects and virtual environments. It handles dependency installation, virtual environment creation, and other project configurations. In this project, <code>UV</code> is used to manage dependencies and the virtual environment inside the Docker container, ensuring a consistent and reproducible setup.</p> <p>See Guides - UV for a comprehensive guide.</p>"},{"location":"#configuration-management","title":"Configuration Management","text":"<p>This project uses a hybrid approach for configuration management:</p> <ol> <li><code>pyproject.toml</code>: Used for project metadata, dependencies incl. groups for development and documentation, and tool-specific configurations, as well as packaging.</li> <li><code>settings.toml</code>: Stores static, non-sensitive application parameters like model hyperparameters, training settings, and data paths. This file is located in the root of the generated project.</li> <li>Environment Variables: Used for dynamic or sensitive settings, overriding values from <code>config.toml</code>.</li> </ol> <p>Settings are loaded using Pydantic, which provides type validation and allows overriding <code>config.toml</code> values with environment variables. This ensures a clear separation of concerns and flexibility for different environments.</p> <pre><code># filepath: pyproject.toml\n[project]\nname = \"uv-datascience-project-template\"\nversion = \"0.1.0\"\ndescription = \"Template Project for Developing Data Science Projects with UV. A new project using this template can be created with cookiecutter.\"\nreadme = \"README.md\"\nlicense = {text = \"MIT\"}\nauthors = [\n    {name = \"David Tiefenthaler\"}\n]\nurls = {repository = \"https://github.com/tiefenthaler/uv-datascience-project-template\"}\nkeywords = [\n    \"data science project\",\n    \"docker\",\n    \"python\",\n    \"template\",\n    \"uv\"\n]\nrequires-python = \"&gt;=3.12.0, &lt;3.13.0\"\ndependencies = [\n    \"fastapi[standard]&gt;=0.115.6\",\n    \"lightning&gt;=2.4.0\",\n    \"pydantic&gt;=2.10.4\",\n    \"torch&gt;=2.4.1\",\n    \"torchvision&gt;=0.20.1\",\n    \"uvicorn&gt;=0.34.0\"\n]\n\n# DEV SETTING\n[dependency-groups]\ndev = [\n    \"cookiecutter&gt;=2.6.0\",\n    \"ipykernel&gt;=6.29.5\",\n    \"jupyterlab&gt;=4.3.1\",\n    \"pre-commit&gt;=4.1.0\",\n    \"pyright&gt;=1.1.398\",\n    \"pytest-cov&gt;=6.0.0\",\n    \"pytest&gt;=8.1.1\",\n    \"ruff&gt;=0.11.2\",\n    \"toml-sort&gt;=0.24.2\",\n    \"uv&gt;=0.5.26\"\n]\ndocs = [\n    \"mkdocs&gt;=1.6.1\",\n    \"mkdocs-include-markdown-plugin&gt;=7.1.4\",\n    \"mkdocs-jupyter&gt;=0.25.1\",\n    \"mkdocs-material&gt;=9.6.3\",\n    \"mkdocstrings-python&gt;=1.16.8\",\n    \"pymdown-extensions&gt;=10.14.3\",\n]\n\n# DEV SETTING\n[tool.uv]\ndefault-groups = [\"dev\"]\n\n# DEV SETTING\n# ruff.toml file is used.\n[tool.ruff]\n\n# DEV SETTING\n# pytest.ini file is used.\n[tool.pytest]\n\n# DEV SETTING\n# .coveragerc file is used. A pytest fixture in .conftest.py is used to create coverage file/report directory.\n[tool.coverage]\n\n# DEV SETTING\n# pyrightconfig.json file is used. NOTE: Ensure to set the python version correctly.\n[tool.pyright]\n\n# DEV SETTING\n# NOTE: to sort, run: \"uv run toml-sort pyproject.toml\"\n[tool.tomlsort]\nin_place = true\nno_sort_tables = true\nsort_inline_arrays = true\nspaces_before_inline_comment = 4\nspaces_indent_inline_array = 4\n\n[tool.hatch.build.targets.wheel]\npackages = [\"src/uv_datascience_project_template\"]\n\n[build-system]\nrequires = [\"hatchling\"]\nbuild-backend = \"hatchling.build\"\n</code></pre>"},{"location":"#cookiecutter-to-create-a-new-project-from-this-template","title":"&gt; Cookiecutter to create a new project from this template","text":"<p>Cookiecutter is a command-line utility that creates projects from project templates. This guide explains how cookiecutter works and how it's used in this project.</p>"},{"location":"#how-to-use-cookiecutter","title":"How to Use Cookiecutter","text":"<ol> <li>Install Cookiecutter</li> <li>Create a new project using the template:    <code>bash    cookiecutter gh:tiefenthaler/uv-datascience-project-template</code></li> <li>Follow the prompts to customize your project:</li> <li>Enter the project name.</li> <li>Enter the project description.</li> <li>...</li> </ol> <ol> <li>Navigate to the newly created project directory:    <pre><code>cd &lt;project_name&gt;\n</code></pre></li> <li>Use the <code>Makefile</code> to manage the project:</li> <li>Create virtual environment and install dependencies:      <pre><code>make install\n</code></pre></li> <li>Run checks for code quality:      <pre><code>make check\n</code></pre></li> <li>Run additional commands like \"test\", \"build\", and others as needed.</li> <li>Run the FastAPI application:      <pre><code>make run-api\n</code></pre></li> <li>Format code:      <pre><code>make format\n</code></pre></li> <li>Lint and fix code:      <pre><code>make lint-fix\n</code></pre></li> <li>Check for template synchronization issues:      <pre><code>make check-template-sync\n</code></pre></li> <li>Happy coding!</li> </ol>"},{"location":"#custom-code-in-src-folder","title":"Custom Code in src Folder","text":"<p>See Source Code API Reference for a comprehensive documentation.</p> <p>The <code>src</code> folder contains the custom code for the machine learning project. The main components include:</p>"},{"location":"#lit_auto_encoder","title":"lit_auto_encoder","text":"<p>This file defines the <code>LitAutoEncoder</code> class, which is a LightningModule an autoencoder using PyTorch Lightning. The <code>LitAutoEncoder</code> class includes:</p> <ol> <li>An <code>__init__</code> method to initialize the encoder and decoder.</li> <li>A <code>training_step</code> method to define the training loop.</li> <li>A <code>configure_optimizers</code> method to set up the optimizer.</li> </ol>"},{"location":"#train_autoencoder","title":"train_autoencoder","text":"<p>This file defines the training function <code>train_litautoencoder</code> to initialize and train the model on the MNIST dataset using PyTorch Lightning. It now returns the path to the saved model checkpoint.</p>"},{"location":"#fastapi-application","title":"FastAPI Application","text":"<p>The FastAPI application is defined in the <code>app_fastapi_autoencoder.py</code> file. It includes the following endpoints:</p> <ol> <li><code>GET /</code>: Root endpoint that provides a welcome message and instructions.</li> <li><code>POST /train</code>: Endpoint to train the autoencoder model.</li> <li><code>POST /embed</code>: Endpoint to embed fake images using the trained autoencoder.</li> </ol>"},{"location":"#app_fastapi_autoencoder","title":"app_fastapi_autoencoder","text":"<p>See Source Code API Reference for a comprehensive documentation.</p> <p>This file defines the FastAPI application and the endpoints. It uses <code>app.state</code> to manage the model's state (encoder, decoder, training status, and checkpoint path) instead of global variables. This provides a cleaner and more scalable pattern. It includes:</p> <ol> <li>Importing necessary libraries and modules.</li> <li>Initializing <code>app.state</code> on application startup to manage model components.</li> <li>A <code>NumberFakeImages</code> class for input validation.</li> <li>A <code>train_litautoencoder</code> function to initialize and train the autoencoder.</li> <li>A <code>read_root</code> function to handle the root endpoint.</li> <li>A <code>train_model</code> function to handle the model training endpoint, which now updates <code>app.state</code>.</li> <li>An <code>embed</code> function to handle the embedding endpoint, which now retrieves the model and checkpoint path from <code>app.state</code>.</li> <li>The application entry point to run the FastAPI application.</li> </ol>"},{"location":"#main","title":"main","text":"<p>This file defines the uvicorn server to run the FastAPI AutoEncoder application and the endpoints. It includes:</p> <ol> <li>Importing necessary libraries and modules, including the source code of the project.</li> <li>The application entry point to run the FastAPI application.</li> </ol> <pre><code># filepath: main.py\n\n# Application entry point\nif __name__ == \"__main__\":\n    # Run the FastAPI application\n    uvicorn.run(app=app, host=\"0.0.0.0\", port=8000)\n</code></pre>"},{"location":"#production-setup-for-the-machine-learning-fastapi-app-hosted-in-the-docker-container","title":"Production Setup for the Machine Learning FastAPI App hosted in the Docker container","text":"<p>See Docker Production Setup for a comprehensive guide.</p>"},{"location":"#dockerfile","title":"Dockerfile","text":"<p>The <code>Dockerfile</code> is used to build the Docker image for the project. It includes the following steps:</p> <ol> <li>Define build-time arguments for the base container images and workspace name.</li> <li>Use a Python image with <code>uv</code> pre-installed.</li> <li>Set the working directory.</li> <li>Enable bytecode compilation for faster startup.</li> <li>Copy and install dependencies without installing the project.</li> <li>Copy the application source code and install it.</li> <li>Add executables and source to environment paths.</li> <li>Set the default command to run the FastAPI application.</li> </ol>"},{"location":"#multi-stage-dockerfile","title":"Multi Stage Dockerfile","text":"<p>To build the multistage image for a container optimized final image without uv use the <code>multistage.Dockerfile</code>.</p>"},{"location":"#docker-compose","title":"Docker Compose","text":"<p>The <code>docker-compose.yml</code> file is used to define and run multi-container Docker applications. It includes the following configurations:</p> <ol> <li>Build the image from the <code>Dockerfile</code>.</li> <li>Define the image name.</li> <li>Host the FastAPI application on port 8000.</li> <li>Mount the current directory to the app directory in the container.</li> <li>Set environment variables.</li> <li>Define the default command to start the FastAPI application.</li> </ol>"},{"location":"#build-the-docker-image-and-run-a-container","title":"Build the docker image and run a container","text":"<p>Build and run a specific or all services when multiple services (\"app\" and \"app-optimized-docker\") are defined in <code>docker-compose.yml</code>. Note that in the give example both services us the same port and only one service at a time should be used.</p> <pre><code>docker-compose up --build\n</code></pre> <p>or to build a single service only \"app\" respectively \"app-optimized-docker\".</p> <pre><code>docker-compose up --build app\n</code></pre> <pre><code>docker-compose up --build app-optimized-docker\n</code></pre>"},{"location":"#test-the-endpoint-with-curl","title":"Test the endpoint with curl","text":"<ul> <li> <p>Welcome root endpoint</p> <pre><code>curl -X GET http://0.0.0.0:8000/\n</code></pre> </li> <li> <p>Get docs of the request options of the FastAPI app:</p> <pre><code>curl -X GET http://0.0.0.0:8000/docs\n</code></pre> </li> <li> <p>Test the endpoint with curl by training the model first, followed by requesting predictions for n fake images</p> <pre><code>curl -X POST http://0.0.0.0:8000/train \\\ncurl -X POST http://0.0.0.0:8000/embed -H \"Content-Type: application/json\" -d '{\"n_fake_images\": 4}'\n</code></pre> </li> </ul>"},{"location":"#development-in-dev-container","title":"Development in Dev Container","text":"<p>See VSCode Dev-Container (Docker) Setup for Data Science Projects using UV for a comprehensive guide.</p> <ul> <li>Run the server: <code>uv run /workspace/main.py</code></li> <li> <p>Test the standard endpoints with curl:</p> <ul> <li> <p>Get docs of the request options of the FastAPI app</p> <pre><code>curl -X GET http://localhost:8000/docs\n</code></pre> </li> <li> <p>Welcome root request of the FastAPI app, providing an app description</p> <pre><code>curl -X GET http://localhost:8000/\n</code></pre> </li> <li> <p>Test the machine learning endpoints with curl:</p> <pre><code>curl -X POST http://localhost:8000/train \\\ncurl -X POST http://localhost:8000/embed -H \"Content-Type: application/json\" -d '{\"n_fake_images\": 1}'\n</code></pre> </li> </ul> </li> </ul>"},{"location":"#conclusion","title":"Conclusion","text":"<p>This repository provides a comprehensive overview of setting up and running the machine learning FastAPI project using Docker and <code>uv</code>. Follow the instructions to build and run the application in both development and production environments. The project demonstrates how to develop and productionize machine learning applications using modern tools and best practices.</p> <p>Additionally, ensure to review the provided guides and documentation for detailed instructions on various setups and configurations necessary for optimal project performance.</p>"},{"location":"about/","title":"About This Project","text":"<p>Welcome to the UV Data Science Project Template! This project is designed to help you get started with data science projects quickly and efficiently by providing a structured, modern, and scalable foundation.</p>"},{"location":"about/#why-use-this-template","title":"Why Use This Template?","text":"<ul> <li>Streamlined Setup: Save time with pre-configured tools and environments.</li> <li>Reproducibility: Ensure consistent results across different systems.</li> <li>Collaboration: Facilitate teamwork with standardized workflows and CI/CD pipelines.</li> <li>Scalability: Transition seamlessly from development to production.</li> </ul>"},{"location":"about/#key-features","title":"Key Features","text":"<ul> <li>Docker Integration: Containerized environments for development and production.</li> <li>Modern Tooling: Includes UV, Ruff, Pyright, and more.</li> <li>Comprehensive Documentation: Guides, API references, and examples to get you started.</li> <li>Example Use Case: A pre-built autoencoder for image compression using PyTorch Lightning and FastAPI.</li> </ul>"},{"location":"about/#how-to-contribute","title":"How to Contribute","text":"<p>We welcome contributions from the community! Here\u2019s how you can get involved:</p> <ol> <li>Fork the repository on GitHub.</li> <li>Create a new branch for your feature or bug fix.</li> <li>Submit a pull request with a detailed description of your changes.</li> </ol> <p>For more details, see the Contributing Guide.</p>"},{"location":"about/#license","title":"License","text":"<p>This project is licensed under the MIT License. See the LICENSE file for more details.</p>"},{"location":"about/#contact","title":"Contact","text":"<p>If you have any questions or feedback, please feel free to reach out via GitHub Issues.</p>"},{"location":"about/#acknowledgments","title":"Acknowledgments","text":"<p>Thank you for using the UV Data Science Project Template! We hope it accelerates your data science journey.</p>"},{"location":"blogpost_uv_data_science_project_template/","title":"Supercharge Your Data Science Projects with the UV Data Science Project Template","text":"<p>Image by David T. [Source: Astral]</p> <p>Data science is a field that thrives on innovation, but even the most groundbreaking ideas can falter without a solid foundation. Setting up a robust, reproducible, and scalable project structure is often a daunting task. Enter the UV Data Science Project Template \u2014 a modern, feature-rich framework designed to streamline your workflow and provide a solid foundation for your projects.</p> <p>In this article, we\u2019ll explore why this template exists, dive into the powerful tools it integrates (like UV or Ruff), examine its structure, and see how it can transform your approach to data science projects.</p> <p>Key Feature: Automatic Project Setup Generation using Cookiecutter.</p> <p>Find the related GitHub Repo and related Docs here.</p> <p> </p> <p>Table of Contents</p> <ul> <li>Supercharge Your Data Science Projects with the UV Data Science Project Template</li> <li>Why Battle Setup? Use the UV Data Science Project Template</li> <li>A Curated Toolkit for Modern Data Science<ul> <li>UV: The Swift Foundation</li> <li>Ruff: Lightning-Fast Linting and Formatting</li> <li>Pyright: Robust Type Checking</li> <li>Pytest and Coverage.py: Ensuring Reliability</li> <li>Pre-Commit Hooks: Quality Control at Commit Time</li> <li>CI with GitHub Actions: Automated Workflows</li> <li>MkDocs: Professional Project Documentation</li> <li>Docker Production: Consistent Environments Anywhere</li> <li>VSCode DevContainer: Seamless Development Environments</li> <li>Usage</li> <li>Benefits</li> </ul> </li> <li>Project Structure: A Place for Everything</li> <li>Example Use Case: Autoencoder for Image Compression<ul> <li>Objective</li> <li>Dataset</li> <li>Implementation</li> <li>FastAPI Integration</li> </ul> </li> <li>Automate Project Kick-off with Cookiecutter<ul> <li>How to Use</li> </ul> </li> <li>Conclusion: Build Better, Faster</li> </ul>"},{"location":"blogpost_uv_data_science_project_template/#why-battle-setup-use-the-uv-data-science-project-template","title":"Why Battle Setup? Use the UV Data Science Project Template","text":"<p>The UV Data Science Project Template is engineered to tackle common challenges during setup and development of data science projects by providing a structured environment that ensures your projects are:</p> <ul> <li>Reproducible by Design: Forget \"it works on my machine.\" By leveraging Docker for containerization and UV for precise dependency management, the template creates isolated, consistent environments every time, everywhere.</li> <li>Collaboration-Ready: Stop reinventing the wheel for team workflows. Pre-configured CI/CD pipelines using GitHub Actions, automated code quality checks with pre-commit hooks, and standardized documentation tools (MkDocs) make teamwork seamless and efficient.</li> <li>Scalable from the Start: Transitioning from development to production shouldn't be an afterthought. With production-ready configurations and tools like Docker, the template is built for deploying and scaling your data science applications.</li> </ul> <p>By adopting this template, you free yourself to focus on what truly matters: extracting insights, building models, and solving complex data problems, rather than wrestling with the project setup.</p>"},{"location":"blogpost_uv_data_science_project_template/#a-curated-toolkit-for-modern-data-science","title":"A Curated Toolkit for Modern Data Science","text":"<p>The UV Data Science Project Template integrates a suite of powerful, best-in-class tools, each chosen to optimize specific parts of the data science project development lifecycle.</p> Tool Description UV A fast and efficient package manager for Python, written in Rust. It replaces tools like pip and virtualenv. Ruff An extremely fast Python linter, formatter, and code assistant, written in Rust. PyRight A static type checker for Python, helping to catch type-related errors early in the development process. PyTest A powerful and flexible testing framework for Python, simplifying writing and running tests. Coverage A tool for measuring code coverage of Python programs, helping to ensure that all parts of the code are tested. Pre-Commit A framework for managing and maintaining multi-language pre-commit hooks to ensure code quality. CI-GitHub Continuous Integration setup using GitHub Actions to automate testing, linting, and deployment. MkDocs A static site generator geared towards building project documentation, written in Markdown. VSCode-DevContainer A development environment setup using Docker and VS Code, providing a consistent and isolated workspace. Docker-Production Docker setup for creating a lean, efficient, and secure production environment for applications. Cookiecutter A command-line utility that creates projects from project templates."},{"location":"blogpost_uv_data_science_project_template/#uv-the-swift-foundation","title":"UV: The Swift Foundation","text":"<p>UV is an extremely fast Python package installer and resolver, written in Rust by Astral (the makers of Ruff). It aims to be a drop-in replacement for tools like <code>pip</code>, <code>pip-tools</code>, and <code>virtualenv</code>.</p> <ul> <li>Blazing Fast Dependency Management: Resolves and installs packages significantly faster than traditional tools.</li> <li>Unified Environment Management: Creates and manages virtual environments seamlessly.</li> <li>Project Packaging: Simplifies building and distributing your Python projects.</li> <li> <p>Usage: Manage dependencies, run scripts, and handle virtual environments effortlessly. For example:</p> <pre><code># Add a new dependency\nuv add pytest\n\n# Install dependencies from pyproject.toml\nuv sync\n\n# Run pytest within the managed environment\nuv run pytest\n</code></pre> </li> </ul> <p>For more details, refer to the UV Guide.</p>"},{"location":"blogpost_uv_data_science_project_template/#ruff-lightning-fast-linting-and-formatting","title":"Ruff: Lightning-Fast Linting and Formatting","text":"<p>Ruff is an incredibly fast Python linter and formatter, also written in Rust. It can replace multiple tools like Flake8, isort, and Black, dramatically speeding up code quality checks.</p> <ul> <li>Comprehensive Code Quality: Enforces style consistency, identifies potential bugs, and sorts imports automatically.</li> <li>Pre-Commit Integration: Ensures code is linted and formatted before it even gets committed.</li> <li> <p>Usage: Keep your codebase clean and consistent with simple commands:</p> <pre><code># Check for linting errors\nuv run ruff check .\n\n# Format the codebase\nuv run ruff format .\n</code></pre> </li> </ul> <p>For configuration details, see the Ruff Guide.</p>"},{"location":"blogpost_uv_data_science_project_template/#pyright-robust-type-checking","title":"Pyright: Robust Type Checking","text":"<p>Pyright, developed by Microsoft, is a fast static type checker for Python. Catching type errors early saves debugging time down the line.</p> <ul> <li>Static Analysis: Verifies type annotations for functions, variables, and data structures, preventing runtime errors.</li> <li> <p>Usage: Integrated into the CI pipeline and runnable locally:</p> <pre><code>uv run pyright\n</code></pre> </li> </ul> <p>Learn more in the Pyright Guide.</p>"},{"location":"blogpost_uv_data_science_project_template/#pytest-and-coveragepy-ensuring-reliability","title":"Pytest and Coverage.py: Ensuring Reliability","text":"<p>Reliable code requires thorough testing. Pytest makes writing tests simple and powerful, while Coverage.py measures your test coverage.</p> <ul> <li>Effective Testing: Write and run unit, integration, or functional tests with Pytest's intuitive framework.</li> <li>Code Coverage Insights: Identify which parts of your code aren't covered by tests, guiding further testing efforts.</li> <li> <p>Usage: Execute tests and get a coverage report:</p> <pre><code>uv run pytest --cov\n</code></pre> </li> </ul> <p>For detailed instructions, refer to the Pytest Guide.</p>"},{"location":"blogpost_uv_data_science_project_template/#pre-commit-hooks-quality-control-at-commit-time","title":"Pre-Commit Hooks: Quality Control at Commit Time","text":"<p>Pre-commit hooks run checks on your code before you commit it, catching issues early and enforcing standards across the team.</p> <ul> <li>Automated Checks: The template includes hooks for Ruff (linting/formatting) and Pyright (type checking).</li> <li> <p>Usage: Install once, and it runs automatically on <code>git commit</code>.</p> <pre><code># Install the hooks\nuv run pre-commit install\n\n# Run hooks manually on all files\nuv run pre-commit run --all-files\n</code></pre> </li> </ul> <p>For setup and customization, refer to the Pre-Commit Guide.</p>"},{"location":"blogpost_uv_data_science_project_template/#ci-with-github-actions-automated-workflows","title":"CI with GitHub Actions: Automated Workflows","text":"<p>GitHub Actions automate your CI/CD (Continuous Integration/Continuous Deployment) pipelines directly within your repository.</p> <ul> <li>Automated Quality Gates: Automatically run linting, type checking, and tests on every push or pull request.</li> <li>Streamlined Deployment: Automate tasks like building documentation and deploying it to GitHub Pages.</li> <li>Usage: Defined in <code>.github/workflows/</code>, these pipelines run automatically based on repository events.</li> </ul> <p>For more information, see the CI Guide.</p>"},{"location":"blogpost_uv_data_science_project_template/#mkdocs-professional-project-documentation","title":"MkDocs: Professional Project Documentation","text":"<p>Clear documentation is crucial for maintainability and collaboration. MkDocs generates a polished static website from your Markdown files.</p> <ul> <li>Effortless Documentation: Write docs in familiar Markdown; MkDocs handles the rest.</li> <li>Helpful Plugins: Includes <code>mkdocstrings</code> to generate API documentation directly from your code's docstrings and <code>mkdocs-material</code> for a modern, responsive theme.</li> <li> <p>Usage: Keep documentation in the <code>docs/</code> folder and preview changes locally:</p> <pre><code>uv run mkdocs serve\n</code></pre> </li> </ul> <p>For setup and deployment, see the MkDocs Guide.</p>"},{"location":"blogpost_uv_data_science_project_template/#docker-production-consistent-environments-anywhere","title":"Docker Production: Consistent Environments Anywhere","text":"<p>Docker containerizes your application, ensuring it runs identically regardless of the underlying system.</p> <ul> <li>Development Consistency: Use the provided <code>docker-compose.yml</code> for an easy-to-set-up, isolated development environment.</li> <li>Optimized Production Images: Leverage the <code>multistage.Dockerfile</code> to build lean, secure images suitable for deployment.</li> <li> <p>Usage: Build and run your application within a container:</p> <pre><code>docker-compose up --build\n</code></pre> </li> </ul> <p>For a detailed breakdown, refer to the Docker Production Guide.</p>"},{"location":"blogpost_uv_data_science_project_template/#vscode-devcontainer-seamless-development-environments","title":"VSCode DevContainer: Seamless Development Environments","text":"<p>The VSCode DevContainer setup simplifies development by providing a pre-configured, containerized environment tailored for data science projects. It ensures consistency across different systems and eliminates the \"it works on my machine\" problem.</p> <ul> <li>Pre-Configured Environment: Includes Python, UV, Docker, and essential VSCode extensions like Python, Jupyter, and Ruff.</li> <li>Volume Mapping: Syncs your local files with the container, enabling real-time updates without rebuilding the image.</li> <li>Multi-Stage Build: Optimizes the container size while maintaining all necessary tools for development.</li> </ul>"},{"location":"blogpost_uv_data_science_project_template/#usage","title":"Usage","text":"<ol> <li> <p>Install Prerequisites:</p> <ul> <li>Install Docker Desktop.</li> <li>Install VSCode and the Remote - Containers extension.</li> </ul> </li> <li> <p>Start the DevContainer:</p> <ul> <li>Open the project in VSCode.</li> <li>Use the command palette (<code>F1</code>) and select \"Open Folder in Container...\".</li> </ul> </li> <li> <p>Run the Application:</p> <ul> <li>Run demo.py in the Python virtual environment: <code>uv run main.py</code>.</li> <li>The FastAPI application is available at <code>http://localhost:8000</code>.</li> <li> <p>Test endpoints using <code>curl</code>:</p> <pre><code>curl -X GET http://localhost:8000/\ncurl -X POST http://localhost:8000/train\ncurl -X POST http://localhost:8000/embed -H \"Content-Type: application/json\" -d '{\"n_fake_images\": 1}'\n</code></pre> </li> </ul> </li> </ol>"},{"location":"blogpost_uv_data_science_project_template/#benefits","title":"Benefits","text":"<ul> <li>Consistency: Ensures all team members work in the same environment.</li> <li>Customization: Easily extend the setup with additional tools or configurations.</li> <li>Integration: Works seamlessly with Docker Compose and GitHub Actions.</li> </ul> <p>For more details, refer to the DevContainer Guide.</p>"},{"location":"blogpost_uv_data_science_project_template/#project-structure-a-place-for-everything","title":"Project Structure: A Place for Everything","text":"<p>The UV Data Science Project Template provides a logical and extensible directory structure:</p> <ul> <li><code>src/</code>: Contains your core Python source code. The example includes:</li> <li>A PyTorch Lightning-based autoencoder model (<code>src/lit_auto_encoder.py</code>).</li> <li>Training logic (<code>src/train_autoencoder.py</code>).</li> <li>A FastAPI application (<code>src/app_fastapi_autoencoder.py</code>) to serve the ML model.</li> <li><code>tests/</code>: Houses unit and integration tests written using Pytest.</li> <li><code>docs/</code>: Holds your project documentation in Markdown format, ready for MkDocs.</li> <li>Configuration Files: Root directory contains pre-configured files like <code>pyproject.toml</code> (for UV, Ruff, Pytest, Pyright), <code>.pre-commit-config.yaml</code>, etc.</li> <li>Docker Support: <code>Dockerfile</code>, <code>multistage.Dockerfile</code>, and <code>docker-compose.yml</code> provide containerization setups for development and production.</li> <li>CI/CD Workflows: <code>.github/workflows/</code> defines the automated GitHub Actions pipelines.</li> <li>Cookiecutter Template: The template structure is designed to be used with Cookiecutter, allowing you to create new projects based on this template easily. Including a Makefile to simplify common tasks like installing dependencies, running tests, and starting the FastAPI server.</li> </ul>"},{"location":"blogpost_uv_data_science_project_template/#example-use-case-autoencoder-for-image-compression","title":"Example Use Case: Autoencoder for Image Compression","text":"<p>To demonstrate the template's practical application, it includes a machine learning use case: training an autoencoder on the MNIST dataset using PyTorch Lightning and serving it via FastAPI.</p>"},{"location":"blogpost_uv_data_science_project_template/#objective","title":"Objective","text":"<p>The autoencoder learns a compressed representation (encoding) of handwritten digit images and reconstructs them, showcasing unsupervised feature learning.</p>"},{"location":"blogpost_uv_data_science_project_template/#dataset","title":"Dataset","text":"<p>The classic MNIST dataset of 28x28 grayscale handwritten digits.</p>"},{"location":"blogpost_uv_data_science_project_template/#implementation","title":"Implementation","text":"<ul> <li><code>LitAutoEncoder</code>: A <code>pytorch_lightning.LightningModule</code> defining the encoder, decoder, and training/validation logic, simplifying PyTorch boilerplate.</li> <li><code>train_litautoencoder</code>: A function orchestrating the data loading and model training process.</li> </ul>"},{"location":"blogpost_uv_data_science_project_template/#fastapi-integration","title":"FastAPI Integration","text":"<p>A simple API built with FastAPI allows interaction with the model:</p> <ul> <li>Train the autoencoder via an API endpoint.</li> <li>Generate embeddings (compressed representations) for new image data.</li> </ul> <p>This demonstrates how the template structure supports integrating model training and API deployment within a single, organized project.</p>"},{"location":"blogpost_uv_data_science_project_template/#automate-project-kick-off-with-cookiecutter","title":"Automate Project Kick-off with Cookiecutter","text":"<p>Manually copying templates is error-prone. This template leverages Cookiecutter to automate the creation of new projects based on its structure.</p> <ul> <li>Consistency: Every new project starts with the same proven structure and tooling.</li> <li>Speed: Skip the repetitive setup and jump straight into development.</li> <li>Customization: Tailor essential project metadata during creation.</li> </ul> <p>Cookiecutter turns starting a best-practice data science project into a simple command-line interaction.</p>"},{"location":"blogpost_uv_data_science_project_template/#how-to-use","title":"How to Use","text":"<ol> <li> <p>Install Cookiecutter (if you haven't already):</p> <pre><code>pip install cookiecutter\n</code></pre> </li> <li> <p>Generate Your Project:</p> <pre><code>cookiecutter gh:tiefenthaler/uv-datascience-project-template\n</code></pre> </li> <li> <p>Customize: Cookiecutter will prompt you for details like project name, author, Python version, etc.</p> </li> <li>Get Started: <code>cd</code> into your newly created project directory and follow the setup instructions in its <code>README.md</code>.</li> <li>Use the <code>Makefile</code> to manage the project:</li> <li> <p>Create virtual environment and install dependencies:</p> <pre><code>make install\n</code></pre> </li> <li> <p>Run checks for code quality:</p> <pre><code>make check\n</code></pre> </li> <li> <p>Run the FastAPI application:</p> <pre><code>make run\n</code></pre> </li> <li> <p>Run help command to see available <code>Makefile</code> options:</p> <pre><code>make help\n</code></pre> </li> <li> <p>Run additional commands like \"test\", \"build\", and others as needed.</p> </li> <li>Happy coding!</li> </ol> <p>For more details, see the Cookiecutter Guide.</p>"},{"location":"blogpost_uv_data_science_project_template/#conclusion-build-better-faster","title":"Conclusion: Build Better, Faster","text":"<p>The UV Data Science Project Template offers a robust, modern foundation for your data science endeavors. By integrating fast, efficient tools like UV and Ruff, promoting best practices like reproducibility and automated testing, and simplifying setup with Cookiecutter, it empowers you to focus on innovation rather than infrastructure.</p> <p>Stop reinventing the project setup wheel. Give your next data science project a significant head start with this template and experience a more streamlined, collaborative, and production-ready workflow.</p> <p>Ready to try it out? Find the template on GitHub.</p>"},{"location":"api/","title":"Source Code API Reference Overview","text":"<p>Welcome to the API Reference section of the UV Data Science Project Template. This section provides detailed documentation for the core components of the project, including the autoencoder, training scripts, and FastAPI application.</p>"},{"location":"api/#api-documentation","title":"API Documentation","text":"<ul> <li>Autoencoder: Learn about the implementation of the autoencoder model, including its architecture and usage.</li> <li>Training: Explore the training scripts and how to train the autoencoder on the MNIST dataset.</li> <li>FastAPI Application: Understand how the FastAPI application integrates with the autoencoder to provide an API for model interaction.</li> </ul>"},{"location":"api/#how-to-use-the-api","title":"How to Use the API","text":"<ol> <li>Navigate to the API Documentation: Use the links above to access detailed information about each component.</li> <li>Run the FastAPI Application: Follow the instructions in the FastAPI Guide to start the application.</li> <li>Test the Endpoints: Use tools like <code>curl</code> or Postman to interact with the API endpoints.</li> </ol>"},{"location":"api/#additional-resources","title":"Additional Resources","text":"<ul> <li>Guides: Step-by-step instructions for setting up and using the project.</li> <li>About: Background information about the project.</li> </ul>"},{"location":"api/autoencoder/","title":"Autoencoder","text":"<p>The LitAutoEncoder is a PyTorch Lightning module designed for unsupervised learning tasks. It consists of an encoder and a decoder network. The autoencoder's role in the project is to learn a compressed, dense representation of the input data (encoding) and then reconstruct the input data from this representation (decoding). This process helps in understanding the underlying structure of the data and is useful for tasks like anomaly detection, data denoising, and dimensionality reduction.</p>"},{"location":"api/autoencoder/#key-features","title":"Key Features","text":"<ul> <li>Encoder: Compresses input data into a latent representation.</li> <li>Decoder: Reconstructs the input data from the latent representation.</li> <li>Loss Function: Mean Squared Error (MSE) is used to measure reconstruction quality.</li> </ul>"},{"location":"api/autoencoder/#autoencoder-class-api","title":"Autoencoder Class API","text":""},{"location":"api/autoencoder/#uv_datascience_project_template.lit_auto_encoder","title":"<code>uv_datascience_project_template.lit_auto_encoder</code>","text":""},{"location":"api/autoencoder/#uv_datascience_project_template.lit_auto_encoder.LitAutoEncoder","title":"<code>LitAutoEncoder(encoder=None, decoder=None, learning_rate=0.001)</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>A simple autoencoder model.</p> PARAMETER DESCRIPTION <code>encoder</code> <p>The encoder component, responsible for encoding input data.</p> <p> TYPE: <code>Optional[Sequential]</code> DEFAULT: <code>None</code> </p> <code>decoder</code> <p>The decoder component, responsible for decoding encoded data.</p> <p> TYPE: <code>Optional[Sequential]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/uv_datascience_project_template/lit_auto_encoder.py</code> <pre><code>def __init__(\n    self,\n    encoder: Optional[nn.Sequential] = None,\n    decoder: Optional[nn.Sequential] = None,\n    learning_rate: float = 1e-3,\n) -&gt; None:\n    super().__init__()\n    self.encoder = (\n        encoder if encoder else nn.Sequential(nn.Linear(784, 64), nn.ReLU(), nn.Linear(64, 3))\n    )\n    self.decoder = (\n        decoder if decoder else nn.Sequential(nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, 784))\n    )\n    self.learning_rate = learning_rate\n</code></pre>"},{"location":"api/autoencoder/#uv_datascience_project_template.lit_auto_encoder.LitAutoEncoder.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configure the Adam optimizer.</p> Source code in <code>src/uv_datascience_project_template/lit_auto_encoder.py</code> <pre><code>def configure_optimizers(self) -&gt; optim.Adam:\n    \"\"\"Configure the Adam optimizer.\"\"\"\n    # optimizer = optim.Adam(self.parameters(), lr=1e-3)\n    optimizer = optim.Adam(self.parameters(), lr=self.learning_rate)\n    return optimizer\n</code></pre>"},{"location":"api/autoencoder/#uv_datascience_project_template.lit_auto_encoder.LitAutoEncoder.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Performs a single training step for the model.</p> PARAMETER DESCRIPTION <code>batch</code> <p>A tuple containing the input data (x) and the corresponding labels (y).</p> <p> TYPE: <code>Tuple[Tensor, Tensor]</code> </p> <code>batch_idx</code> <p>The index of the current batch.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>The computed loss for the current training step.</p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>src/uv_datascience_project_template/lit_auto_encoder.py</code> <pre><code>def training_step(self, batch: Tuple[Tensor, Tensor], batch_idx: int) -&gt; Tensor:\n    \"\"\"Performs a single training step for the model.\n\n    Args:\n        batch (Tuple[Tensor, Tensor]): A tuple containing the input data (x) and\n            the corresponding labels (y).\n        batch_idx (int): The index of the current batch.\n\n    Returns:\n        Tensor: The computed loss for the current training step.\n    \"\"\"\n\n    x, y = batch\n    x = x.view(x.size(0), -1)\n    z = self.encoder(x)\n    x_hat = self.decoder(z)\n    loss = nn.functional.mse_loss(x_hat, x)\n    # Logging to TensorBoard (if installed) by default\n    # self.log(\"train_loss\", loss)\n    return loss\n</code></pre>"},{"location":"api/fastapi_app/","title":"FastAPI App","text":""},{"location":"api/fastapi_app/#uv_datascience_project_template.app_fastapi_autoencoder","title":"<code>uv_datascience_project_template.app_fastapi_autoencoder</code>","text":""},{"location":"api/fastapi_app/#uv_datascience_project_template.app_fastapi_autoencoder.embed","title":"<code>embed(input_data)</code>","text":"<p>Embed fake images using the trained autoencoder.</p> PARAMETER DESCRIPTION <code>input_data</code> <p>Input data containing the number of fake images to embed.</p> <p> TYPE: <code>NumberFakeImages</code> </p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing the embeddings of the fake images.</p> Source code in <code>src/uv_datascience_project_template/app_fastapi_autoencoder.py</code> <pre><code>@app.post(\"/embed\")\ndef embed(input_data: NumberFakeImages) -&gt; dict[str, Any]:\n    \"\"\"Embed fake images using the trained autoencoder.\n\n    Args:\n        input_data (NumberFakeImages): Input data containing the number of fake images to embed.\n\n    Returns:\n        dict[str, Any]: A dictionary containing the embeddings of the fake images.\n    \"\"\"\n    if app.state.encoder is None or app.state.decoder is None:\n        raise HTTPException(\n            status_code=500, detail=\"Model not initialized. Train the model first.\"\n        )\n\n    n_fake_images = input_data.n_fake_images\n\n    if not app.state.checkpoint_path or not os.path.exists(app.state.checkpoint_path):\n        raise HTTPException(\n            status_code=500, detail=\"Checkpoint file not found. Train the model first.\"\n        )\n\n    # Load the trained autoencoder from the checkpoint\n    autoencoder = LitAutoEncoder.load_from_checkpoint(app.state.checkpoint_path)\n    encoder_model = autoencoder.encoder\n    encoder_model.eval()\n\n    # Generate fake image embeddings based on user input\n    # fake_image_batch = rand(n_fake_images, 28 * 28, device=autoencoder.device)\n    fake_image_batch = rand(n_fake_images, settings.model.input_dim, device=autoencoder.device)\n    embeddings = encoder_model(fake_image_batch)\n    # print(\"\u26a1\" * 20, \"\\nPredictions (image embeddings):\\n\", embeddings, \"\\n\", \"\u26a1\" * 20)\n\n    return {\"embeddings\": embeddings.tolist()}\n</code></pre>"},{"location":"api/fastapi_app/#uv_datascience_project_template.app_fastapi_autoencoder.read_root","title":"<code>read_root()</code>","text":"<p>Root endpoint that provides information about the API.</p> Source code in <code>src/uv_datascience_project_template/app_fastapi_autoencoder.py</code> <pre><code>@app.get(\"/\")\ndef read_root() -&gt; Response:\n    \"\"\"Root endpoint that provides information about the API.\"\"\"\n\n    message = \"\"\"\n    \u26a1\u26a1\u26a1 Welcome to the LitAutoEncoder API! \u26a1\u26a1\u26a1\n    - To train the model, send a POST request to '/train' without providing any additional input.\n    - To get encodings for random fake images, POST to '/embed' with JSON input:\n      {'n_fake_images': [1-10]} in the request body.\n    \"\"\"\n    return Response(content=message, media_type=\"text/plain\")\n</code></pre>"},{"location":"api/fastapi_app/#uv_datascience_project_template.app_fastapi_autoencoder.startup_event","title":"<code>startup_event()</code>  <code>async</code>","text":"<p>Initialize FastAPI app state on startup.</p> Source code in <code>src/uv_datascience_project_template/app_fastapi_autoencoder.py</code> <pre><code>@app.on_event(\"startup\")\nasync def startup_event():\n    \"\"\"Initialize FastAPI app state on startup.\"\"\"\n    app.state.encoder = None\n    app.state.decoder = None\n    app.state.is_model_trained = False\n    app.state.checkpoint_path = None\n</code></pre>"},{"location":"api/fastapi_app/#uv_datascience_project_template.app_fastapi_autoencoder.train_model","title":"<code>train_model()</code>","text":"<p>Train the autoencoder model.</p> RETURNS DESCRIPTION <code>dict[str, str]</code> <p>dict[str, str]: A message indicating the training status.</p> Source code in <code>src/uv_datascience_project_template/app_fastapi_autoencoder.py</code> <pre><code>@app.post(\"/train\")\ndef train_model() -&gt; dict[str, str]:\n    \"\"\"Train the autoencoder model.\n\n    Returns:\n        dict[str, str]: A message indicating the training status.\n    \"\"\"\n    if app.state.is_model_trained:\n        return {\"message\": \"Model is already trained.\"}\n\n    encoder, decoder, is_model_trained, checkpoint_path = train_litautoencoder(settings)\n    app.state.encoder = encoder\n    app.state.decoder = decoder\n    app.state.is_model_trained = is_model_trained\n    app.state.checkpoint_path = checkpoint_path\n    return {\"message\": \"Model training completed successfully.\"}\n</code></pre>"},{"location":"api/training/","title":"Training","text":"<p>The training process for the autoencoder involves optimizing the autoencoder to minimize reconstruction error on the input data.</p>"},{"location":"api/training/#steps","title":"Steps","text":"<ol> <li>Data Preparation: Load and preprocess the MNIST dataset.</li> <li>Model Initialization: Instantiate the LitAutoEncoder model with encoder and decoder.</li> <li>Training Loop: Use PyTorch Lightning's Trainer to handle the training process.</li> </ol>"},{"location":"api/training/#example","title":"Example","text":"<pre><code>from uv_datascience_project_template.train_autoencoder import train\n\n# Train the autoencoder\ntrain(data_loader, epochs=10, learning_rate=0.001)\n</code></pre>"},{"location":"api/training/#training-api","title":"Training API","text":""},{"location":"api/training/#uv_datascience_project_template.train_autoencoder","title":"<code>uv_datascience_project_template.train_autoencoder</code>","text":""},{"location":"api/training/#uv_datascience_project_template.train_autoencoder.train_litautoencoder","title":"<code>train_litautoencoder(settings)</code>","text":"<p>Trains a LitAutoEncoder model on the MNIST dataset and returns the trained encoder, decoder, a flag indicating training completion, and the checkpoint path.</p> PARAMETER DESCRIPTION <code>settings</code> <p>The settings object containing model, training, and data configurations.</p> <p> TYPE: <code>Settings</code> </p> RETURNS DESCRIPTION <code>tuple[Sequential, Sequential, Literal[True], str]</code> <p>tuple[Sequential, Sequential, Literal[True], str]: A tuple containing the trained encoder, decoder, a boolean flag indicating that the model has been successfully trained, and the path to the saved model checkpoint.</p> Source code in <code>src/uv_datascience_project_template/train_autoencoder.py</code> <pre><code>def train_litautoencoder(settings: Settings) -&gt; tuple[Sequential, Sequential, Literal[True], str]:\n    \"\"\"Trains a LitAutoEncoder model on the MNIST dataset and returns\n    the trained encoder, decoder, a flag indicating training completion, and the checkpoint path.\n\n    Args:\n        settings (Settings): The settings object containing model, training,\n            and data configurations.\n\n    Returns:\n        tuple[Sequential, Sequential, Literal[True], str]: A tuple containing the trained encoder,\n            decoder, a boolean flag indicating that the model has been successfully trained,\n            and the path to the saved model checkpoint.\n    \"\"\"  # noqa: D205\n\n    # Define the encoder and decoder architecture\n    # encoder = nn.Sequential(nn.Linear(28 * 28, 64), nn.ReLU(), nn.Linear(64, 3))\n    encoder = nn.Sequential(\n        nn.Linear(settings.model.input_dim, settings.model.hidden_dim),\n        nn.ReLU(),\n        nn.Linear(settings.model.hidden_dim, settings.model.latent_dim),\n    )\n    # decoder = nn.Sequential(nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, 28 * 28))\n    decoder = nn.Sequential(\n        nn.Linear(settings.model.latent_dim, settings.model.hidden_dim),\n        nn.ReLU(),\n        nn.Linear(settings.model.hidden_dim, settings.model.input_dim),\n    )\n\n    # Initialize the LitAutoEncoder\n    autoencoder = LitAutoEncoder(encoder, decoder, learning_rate=settings.training.learning_rate)\n\n    # Load the MNIST dataset\n    # Note: Ensure the path is set correctly in settings.data.mnist_data_path\n    # dataset = MNIST(os.getcwd(), download=True, transform=ToTensor())\n    dataset = MNIST(settings.data.mnist_data_path, download=True, transform=ToTensor())\n    train_loader = utils.data.DataLoader(dataset)\n\n    # Initialize the TensorBoard logger\n    logger = CSVLogger(\"lightning_logs\", name=\"LitAutoEncoder\")\n\n    # Define ModelCheckpoint callback\n    checkpoint_dir = os.path.join(\n        settings.data.mnist_data_path, \"lightning_logs\", \"LitAutoEncoder\", \"checkpoints\"\n    )\n    os.makedirs(checkpoint_dir, exist_ok=True)\n    checkpoint_callback = ModelCheckpoint(\n        dirpath=checkpoint_dir, filename=\"autoencoder-{epoch:02d}\", save_last=True\n    )\n\n    # Train the autoencoder\n    # trainer = L.Trainer(limit_train_batches=100, max_epochs=1, logger=logger)\n    trainer = L.Trainer(\n        limit_train_batches=settings.training.limit_train_batches,\n        max_epochs=settings.training.epochs,\n        logger=logger,\n        callbacks=[checkpoint_callback],\n    )\n    trainer.fit(model=autoencoder, train_dataloaders=train_loader)\n\n    # Save a checkpoint explicitly at the end of training\n    final_checkpoint_path = os.path.join(\n        str(settings.data.mnist_data_path),\n        \"lightning_logs\",\n        \"LitAutoEncoder\",\n        \"checkpoints\",\n        \"final_autoencoder.ckpt\",\n    )\n    os.makedirs(os.path.dirname(final_checkpoint_path), exist_ok=True)\n    trainer.save_checkpoint(final_checkpoint_path)\n\n    is_model_trained = True  # Mark model as trained\n\n    # Return the path to the best model checkpoint\n    return autoencoder.encoder, autoencoder.decoder, is_model_trained, final_checkpoint_path\n</code></pre>"},{"location":"guides/","title":"Guides Overview","text":"<p>Welcome to the Guides section of the UV Data Science Project Template documentation. This section provides step-by-step instructions for setting up and using the various components of the project.</p>"},{"location":"guides/#available-guides","title":"Available Guides","text":"Tool Description UV A fast and efficient package manager for Python, written in Rust. It replaces tools like pip and virtualenv. Ruff An extremely fast Python linter, formatter, and code assistant, written in Rust. PyRight A static type checker for Python, helping to catch type-related errors early in the development process. PyTest A powerful and flexible testing framework for Python, simplifying writing and running tests. Coverage A tool for measuring code coverage of Python programs, helping to ensure that all parts of the code are tested. Pre-Commit A framework for managing and maintaining multi-language pre-commit hooks to ensure code quality. CI-GitHub Continuous Integration setup using GitHub Actions to automate testing, linting, and deployment. MkDocs A static site generator geared towards building project documentation, written in Markdown. VSCode-DevContainer A development environment setup using Docker and VS Code, providing a consistent and isolated workspace. Docker-Production Docker setup for creating a lean, efficient, and secure production environment for applications. Cookiecutter A command-line utility that creates projects from project templates."},{"location":"guides/#how-to-use-the-guides","title":"How to Use the Guides","text":"<ol> <li>Choose a Guide: Select the guide that matches your current task or interest.</li> <li>Follow the Steps: Each guide provides detailed, step-by-step instructions.</li> <li>Refer to Additional Resources: Links to related documentation and tools are provided within each guide.</li> </ol>"},{"location":"guides/#additional-resources","title":"Additional Resources","text":"<ul> <li>API Reference: Explore the API documentation.</li> <li>About: Learn more about the project.</li> </ul>"},{"location":"guides/ci_github/","title":"Continuous Integration (CI)","text":"<p>Continuous Integration is a development practice where developers regularly merge their code changes into a central repository, after which automated builds and tests are run. This helps to detect integration errors early and ensures that the codebase remains in a working state.</p> <ul> <li>Continuous Integration (CI)</li> <li>Integration of Workflows and Actions</li> <li>CI Workflows in this Project</li> <li>Custom GitHub Actions<ul> <li><code>setup-python-with-uv</code></li> <li><code>setup-git-config</code></li> </ul> </li> <li>Getting Started: Set Up GitHub Actions and Workflows</li> </ul>"},{"location":"guides/ci_github/#integration-of-workflows-and-actions","title":"Integration of Workflows and Actions","text":"<p>The CI process is orchestrated through GitHub Actions workflows (defined in the <code>.github/workflows</code> directory). Each workflow defines a series of jobs, which in turn consist of steps. These steps can either be individual commands or calls to reusable actions.</p> <p>For example, the <code>test.yml</code> workflow uses the <code>setup-python-with-uv</code> action to set up the Python environment before running tests. Similarly, the <code>ruff.yml</code> workflow uses <code>setup-python-with-uv</code> before running the Ruff linter and formatter.</p> <p>This modular approach allows for a clear and maintainable CI configuration, where common setup tasks are encapsulated in reusable actions, and workflows define the overall CI process.</p>"},{"location":"guides/ci_github/#ci-workflows-in-this-project","title":"CI Workflows in this Project","text":"<p>This project uses GitHub Actions for CI Workflows. Upon each push or pull request to the <code>main</code> branch, the following steps, defined in <code>.github/workflows</code> directory, are executed:</p> <ol> <li> <p>Linting:</p> <ul> <li>Ruff: The code is checked for stylistic errors, potential bugs, and adherence to coding standards using Ruff. This includes both linting and formatting checks.</li> <li>Hadolint: The <code>Dockerfile</code> and <code>.devcontainer/Dockerfile.debug</code> are checked for errors and best practices using Hadolint.</li> </ul> </li> <li> <p>Type Checking:</p> <ul> <li>Pyright: Static type checking is performed using Pyright to catch type-related errors.</li> </ul> </li> <li> <p>Testing:</p> <ul> <li>Pytest: Unit tests are executed using Pytest to verify the correctness of individual components.</li> <li>Pytest-Coverage: Coverage reports are generated and posted as comments on pull requests using Pytest-cov.</li> </ul> </li> <li> <p>Building:</p> <ul> <li>Docker: The project's Docker image is built to ensure that all dependencies are correctly resolved and that the application can be containerized successfully. This includes building both the main <code>Dockerfile</code> and the development container <code>Dockerfile.debug</code>.</li> </ul> </li> <li> <p>Documentation Deployment: The project documentation is automatically built and deployed to GitHub Pages using mkdocs.</p> </li> <li> <p>CI Workflow Status: Provides a summary of the CI workflow status, passed and failed jobs.</p> </li> </ol> <p>You can check the status of the latest CI run on the GitHub repository under the \"Actions\" tab.</p>"},{"location":"guides/ci_github/#custom-github-actions","title":"Custom GitHub Actions","text":"<p>This project utilizes custom GitHub Actions to encapsulate reusable steps within the CI Workflows. These actions are defined in the <code>.github/actions</code> directory.</p>"},{"location":"guides/ci_github/#setup-python-with-uv","title":"<code>setup-python-with-uv</code>","text":"<p>This action sets up a Python environment using the <code>uv</code> package manager. It handles:</p> <ul> <li>Installing <code>uv</code>.</li> <li>Pinning the specified Python version.</li> <li>Caching <code>uv</code> files to speed up subsequent runs.</li> <li>Installing project dependencies using <code>uv sync</code>.</li> </ul>"},{"location":"guides/ci_github/#setup-git-config","title":"<code>setup-git-config</code>","text":"<p>This action configures Git user name and email. It is used to:</p> <ul> <li>Set the Git user name to <code>github-actions[bot]</code>.</li> <li>Set the Git user email to <code>41898282+github-actions[bot]@users.noreply.github.com</code>.</li> </ul>"},{"location":"guides/ci_github/#getting-started-set-up-github-actions-and-workflows","title":"Getting Started: Set Up GitHub Actions and Workflows","text":"<p>To set up GitHub Actions and Workflows in your project, follow these steps:</p> <ul> <li>Create a <code>.github/workflows</code> directory in the root of your repository.</li> <li> <p>Define your workflows in YAML files within this directory.       Example: <code>.github/workflows/pyright.yml</code></p> <pre><code>name: Pyright\n\non:\npush:\nbranches: [main]\npull_request:\nbranches: [main]\n\njobs:\ntype-check:\nruns-on: ubuntu-latest\n\nstrategy:\n      matrix:\n      python-version: [\"3.9\", \"3.10\", \"3.11\", \"3.12\", \"3.13\"]\n\nsteps:\n      - uses: jakebailey/pyright-action@v2\n            with:\n            python-version: ${{ matrix.python-version }}\n</code></pre> </li> <li> <p>Define jobs and steps within each workflow file.</p> </li> <li> <p>Use reusable actions to encapsulate common steps.       You can use existing actions from the GitHub Marketplace or create custom actions.       Example: <code>.github/actions/setup-python-with-uv/action.yml</code></p> <pre><code>name: Install Python with uv\ndescription: |\nThis GitHub Action installs Python using uv.\nIt pins the specified Python version, caches uv files, and installs dependencies.\n\ninputs:\npython-version:\ndescription: Python version\nrequired: true\n\nruns:\nusing: composite\nsteps:\n- name: Install uv\n      uses: astral-sh/setup-uv@v4\n      with:\n      enable-cache: true\n      python-version: ${{ inputs.python-version }}\n\n- name: Install Dependencies\n      run: uv sync --all-groups\n      shell: bash\n</code></pre> </li> <li> <p>Run the pre-commit hooks manually to ensure they are working correctly.       <pre><code>uv run pre-commit run -a\n</code></pre></p> </li> <li> <p>Commit and push your changes to trigger the workflows.       <pre><code>git add .\ngit commit -m \"Set up GitHub Actions for Pyright\"\ngit push origin main\n</code></pre></p> </li> <li>Monitor the Actions tab in your GitHub repository to check the status of your workflows.       Go to the \"Actions\" tab in your GitHub repository to check the status of your CI workflows. You should see the workflows running based on the configuration in your workflow files.</li> </ul> <p>By following these steps, you can set up GitHub Actions for Continuous Integration in your project, ensuring that your codebase is automatically tested and built upon each push or pull request.</p>"},{"location":"guides/cookiecutter/","title":"Cookiecutter Guide","text":"<p>Cookiecutter is a command-line utility that creates projects from project templates. This guide explains how cookiecutter works and how it's used in this project.</p>"},{"location":"guides/cookiecutter/#what-is-cookiecutter","title":"What is Cookiecutter?","text":"<p>Cookiecutter is a Python-based project templating tool that:</p> <ul> <li>Creates projects from templates</li> <li>Handles variable substitution</li> <li>Supports conditional file creation</li> <li>Allows for pre and post-generation hooks</li> </ul>"},{"location":"guides/cookiecutter/#project-configuration","title":"Project Configuration","text":"<p>The main configuration for cookiecutter is in <code>cookiecutter.json</code>. This file defines all variables that can be customized when creating a new project. Key options in this template include:</p> <p>Additionally, the generated project will include a <code>settings.toml</code> file for managing application-specific settings and parameters. These settings can be overridden by environment variables for flexible deployment.</p> <pre><code>{\n    \"project_name\": \"project-name\",\n    \"project_description\": \"A short description of the project.\",\n    \"author_name\": \"Your name (or your organization/company/team)\",\n    \"python_version\": \"3.12\",\n    \"open_source_license\": [\"Not open source\", \"MIT license\", \"BSD license\", \"Apache-2.0\"],\n    \"docs_mkdocs\": [\"y\", \"n\"],\n    \"use_vscode_devcontainer\": [\"y\", \"n\"]\n}\n</code></pre>"},{"location":"guides/cookiecutter/#using-this-template","title":"Using This Template","text":"<p>Install Cookiecutter (see here if you haven't already.</p> <p>To create a new project using this template:</p> <pre><code>cookiecutter gh:tiefenthaler/uv-datascience-project-template\n</code></pre> <p>You'll be prompted to provide values for each variable defined in <code>cookiecutter.json</code>.</p>"},{"location":"guides/cookiecutter/#template-features","title":"Template Features","text":"<p>The template includes hooks that:</p> <ol> <li>Pre-generation (<code>pre_gen_project.py</code>):</li> <li>Validates repository and module names</li> <li> <p>Checks for valid Python identifiers</p> </li> <li> <p>Post-generation (<code>post_gen_project.py</code>):</p> </li> <li>Removes unused files based on your choices</li> <li>Sets up the project structure</li> <li>Configures selected tools</li> </ol>"},{"location":"guides/cookiecutter/#customization-options","title":"Customization Options","text":"<p>The template allows you to customize:</p> <ul> <li>Project structure (<code>directory_structure</code>)</li> <li>Testing setup (<code>testing_framework</code>)</li> <li>Code quality tools (<code>linting_and_formatting</code>, <code>static_type_checking</code>)</li> <li>Documentation (<code>docs_mkdocs</code>)</li> <li>Development environment (<code>use_vscode_devcontainer</code>)</li> <li>Production setup (<code>use_docker_production</code>)</li> </ul>"},{"location":"guides/cookiecutter/#file-generation","title":"File Generation","text":"<p>Based on your choices, the template will:</p> <ol> <li>Generate appropriate configuration files, including an example application that demonstrates best practices for FastAPI state management and model checkpointing.</li> <li>Set up documentation structure if <code>docs_mkdocs=\"y\"</code></li> <li>Configure GitHub Actions if <code>github_actions_ci=\"y\"</code></li> <li>Set up dev containers if <code>use_vscode_devcontainer=\"y\"</code></li> </ol>"},{"location":"guides/cookiecutter/#best-practices","title":"Best Practices","text":"<p>When using this template:</p> <ol> <li>Choose meaningful project and module names</li> <li>Provide a clear project description</li> <li>Review generated files before committing</li> <li>Follow the post-generation setup instructions in the README</li> </ol>"},{"location":"guides/cookiecutter/#additional-features-to-setup-the-project","title":"Additional Features to Setup the Project","text":"<ol> <li>Navigate to the newly created project directory:    <pre><code>cd &lt;project_name&gt;\n</code></pre></li> <li>Use the <code>Makefile</code> to manage the project:</li> <li>Create virtual environment and install dependencies:      <pre><code>make install\n</code></pre></li> <li>Run checks for code quality:      <pre><code>make check\n</code></pre></li> <li>Run additional commands like \"test\", \"build\", and others as needed.</li> <li>Run the FastAPI application:      <pre><code>make run-api\n</code></pre></li> <li>Format code:      <pre><code>make format\n</code></pre></li> <li>Lint and fix code:      <pre><code>make lint-fix\n</code></pre></li> <li>Check for template synchronization issues:      <pre><code>make check-template-sync\n</code></pre></li> <li>Happy coding!</li> </ol>"},{"location":"guides/cookiecutter/#additional-resources","title":"Additional Resources","text":"<ul> <li>Official Cookiecutter Documentation</li> <li>Template Repository</li> </ul>"},{"location":"guides/docker_prod/","title":"Docker Production Setup","text":"<p>This document provides an in-depth guide to the Docker production setup used in this project. It covers the rationale behind the chosen approach, the structure of the <code>Dockerfile</code> respectively <code>multistage.Dockerfile</code> and <code>docker-compose.yml</code> files, and best practices for deploying the application in a production environment.</p> <ul> <li>Docker Production Setup</li> <li>Overview<ul> <li>Key Components</li> </ul> </li> <li><code>Dockerfile</code> Explained<ul> <li>1. Base Image</li> <li>2. Working Directory</li> <li>3. Dependency Installation</li> <li>4. Copying Application Code</li> <li>5. Installing Application</li> <li>6. Environment Variables</li> <li>7. Entrypoint and Command</li> </ul> </li> <li><code>multistage.Dockerfile</code> Explained<ul> <li>1. Builder Stage Image</li> <li>2. Final Image</li> </ul> </li> <li><code>docker-compose.yml</code> Explained<ul> <li>Build Arguments</li> <li>1. Standard Services</li> <li>2. Optimized Docker Service</li> </ul> </li> <li>Build and Run and Test the Docker Application</li> <li>Best Practices</li> <li>Getting Started</li> <li>Conclusion</li> </ul>"},{"location":"guides/docker_prod/#overview","title":"Overview","text":"<p>The Docker setup is designed to create a lean, efficient, and secure production environment for the application. It leverages multi-stage builds to minimize the final image size and uses <code>uv</code> for fast and reliable dependency management.</p>"},{"location":"guides/docker_prod/#key-components","title":"Key Components","text":"<ol> <li> <p><code>Dockerfile</code>: Defines the steps to build the Docker image. It includes:</p> </li> <li> <p>Base image selection.</p> </li> <li>Dependency installation using <code>uv</code>.</li> <li>Copying application code.</li> <li>Setting environment variables.</li> <li> <p>Defining the entry point and command.</p> </li> <li> <p><code>multistage.Dockerfile</code>: Defines an optimized image to reduce image size. Custom python code should be a packaged application.</p> </li> <li> <p><code>docker-compose.yml</code>: Defines the services, networks, and volumes for the application. It includes:</p> </li> <li> <p>Service definitions for the application.</p> </li> <li>Port mappings.</li> <li>Environment variables.</li> <li>Build configurations.</li> </ol>"},{"location":"guides/docker_prod/#dockerfile-explained","title":"<code>Dockerfile</code> Explained","text":"<p>The standard <code>Dockerfile</code> is designed to create a production-ready image that includes all necessary dependencies and the application code. It leverages the uv package manager for efficient dependency management and environment setup. The <code>Dockerfile</code> is structured to optimize for layer caching and minimize the final image size. Here's a breakdown of the key sections:</p>"},{"location":"guides/docker_prod/#1-base-image","title":"1. Base Image","text":"<p>We use a <code>uv</code> pre-installed Python image with a Debian base. The <code>UV_VER</code> argument allows us to specify the Python version at build time.</p> <pre><code>FROM ghcr.io/astral-sh/uv:$UV_VER AS uv\n</code></pre>"},{"location":"guides/docker_prod/#2-working-directory","title":"2. Working Directory","text":"<p>Sets the working directory inside the container.</p> <pre><code>WORKDIR /${WORKSPACE_NAME}\n</code></pre>"},{"location":"guides/docker_prod/#3-dependency-installation","title":"3. Dependency Installation","text":"<pre><code>RUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=uv.lock,target=uv.lock \\\n    --mount=type=bind,source=pyproject.toml,target=pyproject.toml \\\n    uv sync --frozen --no-install-project --no-dev\n</code></pre> <p>Installs the project dependencies using <code>uv sync</code>:</p> <ul> <li><code>--frozen</code> ensures that the exact versions specified in <code>uv.lock</code> are used.</li> <li><code>--no-install-project</code> prevents the project itself from being installed at this stage.</li> <li><code>--no-dev</code> excludes development dependencies.</li> <li><code>--mount=type=cache,target=/root/.cache/uv</code> caches the uv environment.</li> <li><code>--mount=type=bind,source=uv.lock,target=uv.lock</code> binds the uv.lock file to the container.</li> <li><code>--mount=type=bind,source=pyproject.toml,target=pyproject.toml</code> binds the pyproject.toml file to the container.</li> </ul>"},{"location":"guides/docker_prod/#4-copying-application-code","title":"4. Copying Application Code","text":"<p>Copies the application code into the container.</p> <pre><code>COPY . /${WORKSPACE_NAME}\n</code></pre>"},{"location":"guides/docker_prod/#5-installing-application","title":"5. Installing Application","text":"<p>Installs the project itself. <code>--mount=type=cache,target=/root/.cache/uv</code> caches the uv environment.</p> <pre><code>RUN --mount=type=cache,target=/root/.cache/uv \\\n    uv sync --frozen --no-dev\n</code></pre>"},{"location":"guides/docker_prod/#6-environment-variables","title":"6. Environment Variables","text":"<p>Adds the virtual environment's <code>bin</code> directory to the <code>PATH</code> environment variable, ensuring that the correct executables are used.</p> <pre><code>ENV PATH=\"/${WORKSPACE_NAME}/.venv/bin:$PATH\"\n</code></pre>"},{"location":"guides/docker_prod/#7-entrypoint-and-command","title":"7. Entrypoint and Command","text":"<p>Sets the default command to run when the container starts, which in this case is starting the FastAPI application using <code>uvicorn</code>.</p> <pre><code>ENTRYPOINT []\nCMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n</code></pre>"},{"location":"guides/docker_prod/#multistagedockerfile-explained","title":"<code>multistage.Dockerfile</code> Explained","text":"<p>The <code>multistage.Dockerfile</code> is designed to create an optimized production image by separating the build and runtime environments. This approach reduces the final image size and ensures that only the necessary components are included in the runtime image. Here's a breakdown of the key sections:</p>"},{"location":"guides/docker_prod/#1-builder-stage-image","title":"1. Builder Stage Image","text":"<p>Uses the <code>uv</code> image to build the application and install dependencies including the packaged project application.</p> <pre><code>FROM ghcr.io/astral-sh/uv:$UV_VER AS builder\nENV UV_COMPILE_BYTECODE=1 UV_LINK_MODE=copy\nWORKDIR /${WORKSPACE_NAME}\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=uv.lock,target=uv.lock \\\n    --mount=type=bind,source=pyproject.toml,target=pyproject.toml \\\n    uv sync --frozen --no-install-project --no-dev\nCOPY . /${WORKSPACE_NAME}\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv sync --frozen --no-dev\n</code></pre>"},{"location":"guides/docker_prod/#2-final-image","title":"2. Final Image","text":"<p>Uses a slim Python image for the runtime environment and copies the built application from the builder stage.</p> <pre><code>FROM python:3.12-slim-bookworm\nCOPY --from=builder --chown=${WORKSPACE_NAME}:${WORKSPACE_NAME} /${WORKSPACE_NAME} /${WORKSPACE_NAME}\nENV PATH=\"/${WORKSPACE_NAME}/.venv/bin:$PATH\"\nCMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", 8000]\n</code></pre>"},{"location":"guides/docker_prod/#docker-composeyml-explained","title":"<code>docker-compose.yml</code> Explained","text":"<p>The <code>docker-compose.yml</code> file defines the services, networks, and volumes for the application. Here's a breakdown of the key sections:</p>"},{"location":"guides/docker_prod/#build-arguments","title":"Build Arguments","text":"<p>Defines reusable build arguments for the Dockerfile.</p> <pre><code>x-args: &amp;default-args\n  WORKSPACE_NAME: \"app\"\n  UV_VER: \"python3.12-bookworm\"\n</code></pre>"},{"location":"guides/docker_prod/#1-standard-services","title":"1. Standard Services","text":"<p>Builds and runs the application using the standard Dockerfile.</p> <ul> <li>Defines the <code>app</code> service, which is built from the <code>Dockerfile</code> in the current directory.</li> <li>The <code>args</code> section passes build arguments to the <code>Dockerfile</code>.</li> <li>The <code>ports</code> section maps port 8000 on the host to port 8000 on the container.</li> <li>The <code>command</code> section specifies the command to run when the container starts.</li> </ul> <pre><code>services:\n  app:\n    build:\n      context: .\n      dockerfile: Dockerfile\n      args:\n        &lt;&lt;: *default-args\n    image: lit_autoencoder_app\n    ports:\n      - \"8000:8000\"\n    command: [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n</code></pre>"},{"location":"guides/docker_prod/#2-optimized-docker-service","title":"2. Optimized Docker Service","text":"<p>Builds and runs the application using the multi-stage Dockerfile.</p> <ul> <li>Defines the <code>app-optimized-docker</code> service, which is built from the <code>multistage.Dockerfile</code> in the current directory.</li> <li>The <code>args</code> section passes build arguments to the <code>multistage.Dockerfile</code>.</li> <li>The <code>ports</code> section maps port 8000 on the host to port 8000 on the container.</li> <li>The <code>command</code> section specifies the command to run when the container starts.</li> </ul> <pre><code>services:\n app-optimized-docker:\n    build:\n      context: .\n      dockerfile: multistage.Dockerfile\n      args:\n        &lt;&lt;: *default-args\n    image: lit_autoencoder_app\n    ports:\n      - \"8000:8000\"\n    command: [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n</code></pre>"},{"location":"guides/docker_prod/#build-and-run-and-test-the-docker-application","title":"Build and Run and Test the Docker Application","text":""},{"location":"guides/docker_prod/#build-the-docker-image-and-run-a-container","title":"Build the docker image and run a container","text":"<p>Build and run a specific or all services when multiple services (\"app\" and \"app-optimized-docker\") are defined in <code>docker-compose.yml</code>. Note that in the give example both services us the same port and only one service at a time should be used.</p> <pre><code>docker-compose up --build\n</code></pre> <p>or to build a single service only \"app\" respectively \"app-optimized-docker\".</p> <pre><code>docker-compose up --build app\n</code></pre> <pre><code>docker-compose up --build app-optimized-docker\n</code></pre>"},{"location":"guides/docker_prod/#test-the-endpoint-with-curl","title":"Test the endpoint with curl","text":"<ul> <li> <p>Welcome root endpoint</p> <pre><code>curl -X GET http://0.0.0.0:8000/\n</code></pre> </li> <li> <p>Get docs of the request options of the FastAPI app:</p> <pre><code>curl -X GET http://0.0.0.0:8000/docs\n</code></pre> </li> <li> <p>Test the endpoint with curl by training the model first, followed by requesting predictions for n fake images</p> <pre><code>curl -X POST http://0.0.0.0:8000/train \\\ncurl -X POST http://0.0.0.0:8000/embed -H \"Content-Type: application/json\" -d '{\"n_fake_images\": 4}'\n</code></pre> </li> </ul>"},{"location":"guides/docker_prod/#best-practices","title":"Best Practices","text":"<ul> <li>Use Multi-Stage Builds: Multi-stage builds help to reduce the final image size by only including the necessary artifacts.</li> <li>Use <code>.dockerignore</code>: Exclude unnecessary files and directories from the Docker build context to improve build performance and reduce image size.</li> <li>Minimize Layers: Combine multiple commands into a single <code>RUN</code> command to reduce the number of layers in the image.</li> <li>Use Non-Root User: Run the application as a non-root user to improve security.</li> <li>Use Environment Variables: Use environment variables to configure the application at runtime.</li> <li>Use a Volume: Use a volume to persist data between container restarts.</li> </ul>"},{"location":"guides/docker_prod/#getting-started","title":"Getting Started","text":"<ol> <li>Install Docker Compose: Ensure that Docker Compose is installed on your system. It often comes bundled with Docker Desktop for Mac and Windows.</li> <li>Clone the Repository: Clone the project repository to your local machine, so you have access to the Docker configuration files.</li> <li>Build the Docker Image: Build the Docker image using the provided <code>Dockerfile</code> or <code>multistage.Dockerfile</code>. To build (or rebuild) your service, use: <code>docker-compose build</code></li> <li>Run the Application: Start the application using Docker Compose. To start your service, use: <code>docker-compose up</code></li> <li>Access the Application: Open your web browser and go to <code>http://localhost:8000</code> to view the running application.</li> </ol>"},{"location":"guides/docker_prod/#conclusion","title":"Conclusion","text":"<p>This document provides a comprehensive overview of the Docker production setup used in this project. By following the guidelines and best practices outlined in this document, you can create a lean, efficient, and secure production environment for your application.</p>"},{"location":"guides/docker_vscode_devcontainer/","title":"Docker-VSCode-DevContainer","text":""},{"location":"guides/docker_vscode_devcontainer/#vscode-dev-container-setup-for-data-science-projects-using-uv","title":"VSCode Dev-Container Setup for Data Science Projects using UV","text":"<p>This is a containerized dev setup respectively related for using remote containers to work on data science / machine learning projects using UV, Git with VSCode.</p> <p>The repository contains a setup of a local development container using docker compose and VS Code to develop data science projects with UV in a consistent and robust but yet in a simple and customizable way. The Repo provides the configurations and installations for your container, so you can straight get started with your data science work while enjoying the benefits of using docker containers.</p> <p>Find the Documentation about this VSCode Dev-Container for Data Science.</p> <ul> <li>VSCode Dev-Container Setup for Data Science Projects using UV</li> <li>Dev Container Main Configurations and Installations<ul> <li>Setup organization within: Dockerfile.debug | docker-compose.yml | devcontainer.json</li> </ul> </li> <li>Getting Started</li> <li>VS Code Extensions for Data Science Projects<ul> <li>Additional python packages for development</li> <li>Additional python packages for project and code documentation</li> <li>Additional tools to support development</li> </ul> </li> </ul>"},{"location":"guides/docker_vscode_devcontainer/#dev-container-main-configurations-and-installations","title":"Dev Container Main Configurations and Installations","text":"<p>Using a Python image with UV pre-installed, which includes Python, as well as a Debian base image including the VS Code devcontainer base image. Multi-Stage Build: Despite using two base images, only one container is built. The final container is based on the second image, while copying content from the UV stage into the final stage.</p> <ul> <li>UV, an extremely fast Python package, virtual environment and project manager.<ul> <li>\ud83d\ude80 A single tool to replace pip, pip-tools, pipx, poetry, pyenv, twine, virtualenv, and more.</li> <li>\u26a1\ufe0f 10-100x faster than pip.</li> <li>\ud83d\udc0d Installs and manages Python versions.</li> <li>\ud83d\udee0\ufe0f Runs and installs Python applications.</li> <li>\u2747\ufe0f Runs single-file scripts, with support for inline dependency metadata.</li> <li>\ud83d\uddc2\ufe0f Provides comprehensive project management, with a universal lockfile.</li> <li>\ud83d\udd29 Includes a pip-compatible interface for a performance boost with a familiar CLI.</li> <li>\ud83c\udfe2 Supports Cargo-style workspaces for scalable projects.</li> </ul> </li> <li>Volume Mapping: A volume will be used to map a directory on your local file system to a directory inside the Docker container. This way, any changes you make to your code locally will be immediately reflected inside the container, where you can run and test the code.</li> <li>Git: A distributed version control system that tracks changes in any set of computer files, usually used for coordinating work among programmers who are collaboratively developing source code during software development.</li> <li>VS Code, including extensions like Python, Jupyter Notebooks, Docker, PyLance, Ruff and more. Find my list of VS Code Extensions for Data Science Projects in this section below.</li> </ul>"},{"location":"guides/docker_vscode_devcontainer/#setup-organization-within-dockerfiledebug-docker-composeyml-devcontainerjson","title":"Setup organization within: Dockerfile.debug | docker-compose.yml | devcontainer.json","text":"<p>Those files define the dev container setup for data science projects. The list below gives a short overview of the single files. Details can be found in the comments in the single files.</p> <ul> <li> <p>Dockerfile.debug: This file defines the base image for your development container and the steps needed to configure it. Think of it as the blueprint for your container's operating system, dependencies, and tools. It's the foundation upon which everything else is built.</p> Click to toggle contents of the <code>Dockerfile</code> <pre><code># Dockerfile for development purposes.\n# ------------------------------------\n# Use a python image with uv pre-installed and a Debian base image including the VS Code devcontainer base image.\n# To use the image without VS CODE IDE, add lines as indicated (adjust docker-compose.yml as well as documented).\n# ---------------------------------\n\n# Define a build-time argument with a default value for base container images.\nARG UV_VER=0.5.24\nARG DEBIAN_VER=bookworm\nARG WORKSPACE_NAME_=workspace\nARG PROJECT_NAME_=${PROJECT_NAME}\n\n# Multi-Stage Build: Despite using two base images, only one container is built and run.\n# The final container is based on the second image, while copying content from the uv stage into the final stage.\n# FROM ghcr.io/astral-sh/uv:python3.12-bookworm\nFROM ghcr.io/astral-sh/uv:$UV_VER AS uv\n\nFROM mcr.microsoft.com/vscode/devcontainers/base:$DEBIAN_VER\n\n# Install/Update linux packages; install common dev tools like: git, process tools, ...\n# hadolint ignore=DL3008\nRUN apt-get update \\\n    &amp;&amp; apt-get install -y --no-install-recommends\\\n    procps \\\n    build-essential \\\n    curl \\\n    swig \\\n    wget \\\n    # To reduce the image size, it is recommended refresh the package cache as follows.\n    &amp;&amp; apt-get clean \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Copies files or directories from the uv stage into the final stage,\n# and ensures that the ownership of the copied files is adjusted to the user and group in the final image,\n# and making its functionality or binaries available in the final container.\nCOPY --from=uv --chown=vscode: /uv /uvx /bin/\n\nWORKDIR /vscode/${WORKSPACE_NAME_}\n\n# The code to run when container is started:\n# Common practice to keep the Docker container running without performing any significant action.\nENTRYPOINT [\"tail\", \"-f\", \"/dev/null\"]\n</code></pre> </li> <li> <p>docker-compose.yml: (Optional, but often used) This file is used when you have multiple containers that need to work together for your development environment. It defines how these containers interact, their dependencies, and their network configuration. Even if only a single service is used, it defines the additional configuration specific for your data science projects like volume mapping or ports which is optional and therefore not defined in the Dockerfile.debug. This enables the dev container setup to be used with other IDEs then VS Code.</p> Click to toggle contents of the <code>docker-compose.yml</code> <pre><code># Dev Container Configuration File.\n# ---------------------------------\n# Standard Configuration for the service to be used to develop data science applications.\n# Using bind mounts instead of watch for development to sync changes made in the container back to the host.\n# This file depends on a .env file in the root directory of the dev container for dynamic variable interpolation.\n# - The .env file is automatically loaded per default by Docker Compose and is not passed to the container during build.\n# ---------------------------------\n\n# The x-args section defines a reusable set of arguments using YAML anchors.\n# - BUILD arguments (\"UV_VER\", \"DEBIAN_VER\" and \"WORKSPACE_NAME\") to pass to Dockerfile.\nx-args: &amp;default-args\n  UV_VER: \"0.5.5\"\n  DEBIAN_VER: \"bookworm\"\n  WORKSPACE_NAME_: ${WORKSPACE_NAME}\n  PROJECT_NAME_: ${PROJECT_NAME}\n\nservices: # Top level element to configure the arguments of multiple services.\n  myproject: # \"project\" refers to the name of your project/application for which configurations are defined.\n    build: # Tells Docker Compose to build the Docker image using the Dockerfile in the specified directory.\n      context: .\n      dockerfile: ./Dockerfile.debug\n      # Build argument (passed to Dockerfile only)\n      args:\n        &lt;&lt;: *default-args # The &lt;&lt;: *default-args syntax merges the default-args into the args section of the build configuration.\n    image: \"${DEV_USER}.dev-container-uv.${PROJECT_NAME}\" # Explicit way to define the image name\n\n    # Host the FastAPI application on port 8000.\n    ports:\n      - \"8000:8000\"\n\n    # Volumes are persistent data stores (outside container), mounted to be usable by the container.\n    volumes:\n      # Mount the current directory to ${WORKSPACE_NAME} so code changes don't require an image rebuild. .venv is excluded in the .dockerignore file.\n      - type: bind\n        source: ..\n        target: /vscode/${WORKSPACE_NAME}\n      # Mount the virtual environment separately, so the developer's environment doesn't end up in the container.\n      - type: volume\n        source: venv\n        target: /vscode/${WORKSPACE_NAME}/.venv\n\n    working_dir: /vscode/${WORKSPACE_NAME}\n\n    # Runtime environment variable, passed to devcontainer.json. Not available for volumes, networks, or build arguments.\n    # Since docker detects a .env file during build per default, the .env file will be loaded anyways.\n    # Set explicitly for clarity to indicate that the environment variables are used.\n    env_file:\n      - path: .env\n        required: true\n\n    # Default command to start the dev container.\n    command:\n      - sh -c \"chmod -R 777 /vscode/${WORKSPACE_NAME} &amp;&amp; tail -f /dev/null\" # Set permissions on the working directory for root user.\n      - docker-compose down # Remove the container after exiting.\n\n# Define the volumes of the docker container.\nvolumes:\n  venv: # Volume for the virtual environment for persistent in the container.\n</code></pre> </li> <li> <p>devcontainer.json: It configures VS Code (or other supporting IDEs) to use the Docker container as your development environment. It links the Dockerfile (or docker-compose.yml) to VS Code and defines various settings like VS Code extensions or the default environment used by VS Code that are only related to use the given IDE.</p> Click to toggle contents of the <code>devcontainer.json</code> <pre><code>// UV | VS Code - Setup.\n//---------------------\n// This config is set up to be only specific to VS Code.\n// Other configs that do not relate to VS Code are defined in the docker-compose.yml file.\n// This enables the dev container setup to be used with other IDEs, ignoring this file.\n//---------------------\n// Default and dynamic properties for the devcontainer setup:\n// - Default service: \"myproject\", relates to the service defined in the docker-compose.yml.\n// - Default \"workspaceFolder\" is set to \"workspace\" (within the \"/vscode/\" folder in the container).\n// For format details, see https://aka.ms/devcontainer.json.\n//---------------------\n\n{\n  \"name\": \"${localEnv:LOGNAME}.dev-container-uv.${localWorkspaceFolderBasename}\",\n  // Build image using docker compose based on build specs in docker-compose.yml\n  \"dockerComposeFile\": [\"./docker-compose.yml\"],\n  \"service\": \"myproject\",\n  \"runServices\": [\"myproject\"],\n  \"workspaceFolder\": \"/vscode/workspace\",\n  \"postCreateCommand\": {\n    \"uv-sync--frozen\": \"uv sync --frozen --no-binary-package ${localWorkspaceFolderBasename}\" // By default, uv installs projects and workspace members in editable mode, such that changes to the source code are immediately reflected in the environment.\n  },\n  \"postStartCommand\": {\n    // Optional: If applicable, add the following lines for installations.\n    \"uv-run-pre-commit-install\": \"uv run pre-commit install\"\n  },\n  \"features\": {\n        \"ghcr.io/dhoeric/features/hadolint:1\": {}\n    },\n  \"customizations\": {\n    // When connecting to a docker container your local VS Code starts an instance without extensions to ensure isolation and consistency.\n    // Therefore extensions can be specified here for automatic installation when connecting.\n    \"vscode\": {\n      \"settings\": {\n        // Define terminal shell for Dev Container.\n        \"terminal.integrated.profiles.linux\": {\n          \"bash\": {\n            \"path\": \"/bin/bash\"\n          }\n        },\n        \"jupyter.notebookFileRoot\": \"${workspaceRoot}\",\n        \"python.pythonPath\": \"/home/vscode/workspace/.venv/bin/python\"\n      },\n      // Use the VS Code Extensions \"Identifier\" to define extensions.\n      \"extensions\": [\n        // Python\n        \"ms-python.python\",\n        \"ms-toolsai.jupyter\",\n        // Docker\n        \"ms-azuretools.vscode-docker\",\n        \"ms-vscode-remote.remote-containers\",\n        \"ms-vscode-remote.remote-ssh-edit\",\n        \"ms-vscode-remote.remote-ssh\",\n        \"exiasr.hadolint\",\n        // Formatting and Linting\n        \"charliermarsh.ruff\",\n        \"davidanson.vscode-markdownlint\",\n        \"xshrim.txt-syntax\",\n        \"tamasfe.even-better-toml\",\n        \"streetsidesoftware.code-spell-checker\",\n        \"ms-pyright.pyright\",\n        \"ms-python.vscode-pylance\",\n        // Data\n        \"alexcvzz.vscode-sqlite\",\n        \"grapecity.gc-excelviewer\",\n        \"mechatroner.rainbow-csv\",\n        \"zainchen.json\",\n        \"yzane.markdown-pdf\",\n        \"ms-toolsai.datawrangler\",\n        \"yzhang.markdown-all-in-one\",\n        // Cloud\n        //\"ms-azuretools.vscode-docker\",\n        // Git\n        \"github.remotehub\",\n        // AI Coding Assistant\n        \"github.copilot\",\n        \"github.copilot-chat\",\n        // Other\n        \"richie5um2.vscode-sort-json\",\n        \"oliversen.chatgpt-docstrings\"\n      ]\n    }\n  },\n  \"remoteUser\": \"vscode\"\n}\n</code></pre> </li> </ul>"},{"location":"guides/docker_vscode_devcontainer/#getting-started","title":"Getting Started","text":"<ul> <li>Install Docker Desktop.</li> <li>Install VSCode.</li> <li>Install VSCode Extensions.<ul> <li>Install VSCode remote container extension: ms-vscode-remote.remote-containers.</li> </ul> </li> <li>Start dev container:  F1 + \"Open folder in container ...\".</li> <li> <p>To try the setup:</p> <ul> <li>Clone the repository.</li> <li>Run demo.py in the Python virtual environment (<code>uv run main.py</code>).</li> <li> <p>Your application will be available at http://localhost:8000.</p> <ul> <li> <p>Welcome root request of the FastAPI app, providing an app description.</p> <pre><code>curl -X GET http://localhost:8000/\n</code></pre> </li> <li> <p>Test the machine learning endpoints with curl:</p> <pre><code>curl -X POST http://localhost:8000/train \\\ncurl -X POST http://localhost:8000/embed -H \"Content-Type: application/json\" -d '{\"n_fake_images\": 1}'\n</code></pre> </li> </ul> </li> <li> <p>For more information about the application, see the README in the root directory of the repository.</p> </li> </ul> </li> </ul> <p>Optional Steps:</p> <ul> <li>Change the Docker Container in Dockerfile and/or docker-compose.yml.</li> <li>Add/remove VSCode settings and extensions in .devcontainer/devcontainer.json.</li> <li>Update .gitignore and other config files to match your setup.</li> </ul>"},{"location":"guides/docker_vscode_devcontainer/#vs-code-extensions-for-data-science-projects","title":"VS Code Extensions for Data Science Projects","text":"<p>The below extensions are installed for VS Code in the dev container, as defined in <code>devcontainer.json</code>.</p> <pre><code>alexcvzz.vscode-sqlite # SQLite query execution and browsing\ncharliermarsh.ruff # Python linter and code formatter\ndavidanson.vscode-markdownlint # Markdown linter\nexiasr.hadolint # A Dockerfile linter\ngithub.copilot # AI-powered code completion\ngithub.copilot-chat # Chat with your AI coding assistant\ngithub.remotehub # Manage your GitHub repositories\ngrapecity.gc-excelviewer # Excel file viewer\nhediet.vscode-drawio # Draw.io integration\njeff-hykin.polacode-2019 # Generate code blocks from images\nmechatroner.rainbow-csv # Colorizes CSV files for better readability\nms-azuretools.vscode-docker # Docker extension for VS Code\nms-pyright.pyright # Static type checker\nms-python.python # Python language support\nms-python.vscode-pylance # Python language server for enhanced code analysis\nms-toolsai.datawrangler # Data transformation and cleaning tool\nms-toolsai.jupyter # Jupyter Notebooks\nms-pyright.pyright # Static type checker\noliversen.chatgpt-docstrings\nrichie5um2.vscode-sort-json # Sorts JSON objects\nstreetsidesoftware.code-spell-checker # Spell checker for multiple languages\ntamasfe.even-better-toml # Enhanced TOML language support\ntomoki1207.pdf # PDF viewer\nxshrim.txt-syntax # Syntax highlighting for plain text files\nzainchen.json # JSON language support (likely provides syntax highlighting, validation, etc.)\nyzane.markdown-pdf # Generate PDF from Markdown files\nyzhang.markdown-all-in-one # All-in-one Markdown extension\n</code></pre>"},{"location":"guides/docker_vscode_devcontainer/#additional-python-packages-for-development","title":"Additional python packages for development","text":"<ul> <li>uv</li> <li>ruff</li> <li>pytest</li> <li>pytest-cov</li> <li>pre-commit</li> <li>pyright</li> <li>ipykernel</li> <li>jupyterlab</li> <li>toml-sort</li> </ul>"},{"location":"guides/docker_vscode_devcontainer/#additional-python-packages-for-project-and-code-documentation","title":"Additional python packages for project and code documentation","text":"<ul> <li>mkdocs</li> <li>mkdocs-include-markdown-plugin</li> <li>mkdocs-jupyter</li> <li>mkdocs-material</li> <li>mkdocstrings[python]</li> </ul>"},{"location":"guides/docker_vscode_devcontainer/#additional-tools-to-support-development","title":"Additional tools to support development","text":"<ul> <li>GitHub</li> <li>GitHub Actions and Workflows for CI</li> <li>GitHub Pages (to host your documentation)</li> </ul>"},{"location":"guides/mkdocs/","title":"MkDocs Documentation","text":"<p>MkDocs is a fast, simple and downright gorgeous static site generator that's geared towards building project documentation. Documentation source files are written in Markdown, and configured with a single YAML configuration file.</p> <ul> <li>MkDocs Documentation</li> <li>Installation</li> <li>Configuration</li> <li>Using mkdocstrings</li> <li>Using mkdocs-jupyter</li> <li>Create Documentation<ul> <li>Init Documentation</li> <li>Create MkDocs Configuration File</li> <li>Generate General Documentation</li> <li>Generate API Documentation</li> </ul> </li> <li>Building the Documentation</li> <li>Serving the Documentation</li> <li>Deploying the Documentation</li> </ul>"},{"location":"guides/mkdocs/#installation","title":"Installation","text":"<p>Install MkDocs and the related extensions (using uv):</p> <ul> <li>mkdocs</li> <li>mkdocs-include-markdown-plugin</li> <li>mkdocs-material</li> <li>mkdocstrings</li> <li>mkdocs-jupyter</li> </ul>"},{"location":"guides/mkdocs/#configuration","title":"Configuration","text":"<p>The <code>mkdocs.yml</code> file is the configuration file for MkDocs. It contains settings such as the site name, navigation, theme, and plugins.</p> <p>Key Configuration Options Used in this Project:</p> <ul> <li><code>site_name</code>: The name of the site.</li> <li><code>nav</code>: The navigation structure of the site. This defines the table of contents.</li> <li><code>theme</code>: The theme used for the site. We use the material theme.</li> <li><code>plugins</code>: A list of plugins used by MkDocs.<ul> <li><code>search</code>: The built-in search plugin adds a search box to the site.</li> <li><code>include-markdown</code>: This plugin allows you to include Markdown files within other Markdown files. This is useful for reusing content across multiple pages.</li> <li><code>mkdocstrings</code>: This plugin generates documentation from Python docstrings. It is used to generate the \"Source Code API Reference\" section in the navigation.</li> <li><code>mkdocs-jupyter</code>: This plugin allows you to include Jupyter notebooks in your MkDocs documentation.</li> </ul> </li> <li><code>extra_css</code>:<ul> <li><code>stylesheets/extra.css</code>: Custom CSS file to style the documentation.</li> </ul> </li> </ul>"},{"location":"guides/mkdocs/#using-mkdocstrings","title":"Using mkdocstrings","text":"<p>The mkdocstrings plugin is configured to use the google docstring style. This means that the plugin will parse docstrings that follow the Google docstring format. For example:</p> <pre><code>def my_function(arg1, arg2):\n    \"\"\"\n    This is a function that does something.\n\n    Args:\n        arg1: The first argument.\n        arg2: The second argument.\n\n    Returns:\n        The result of the function.\n    \"\"\"\n    return arg1 + arg2\n</code></pre> <p>The mkdocstrings plugin will automatically generate documentation for this function, including the arguments, return value, and description. The \"Source Code API Reference\" section in the navigation is generated using this plugin.</p>"},{"location":"guides/mkdocs/#using-mkdocs-jupyter","title":"Using mkdocs-jupyter","text":"<p>The mkdocs-jupyter plugin allows you to include Jupyter notebooks in your MkDocs documentation. To include a notebook, simply add it to the nav section of your mkdocs.yml file. The plugin will automatically convert the notebook to HTML and include it in your documentation. This is very useful for tutorials. For example:</p> <pre><code>nav:\n  - Notebook: notebook.ipynb\n</code></pre>"},{"location":"guides/mkdocs/#create-documentation","title":"Create Documentation","text":""},{"location":"guides/mkdocs/#init-documentation","title":"Init Documentation","text":"<p>Init documentation using <code>mkdocs</code> for an existing project:</p> <pre><code>uv run mkdocs new .\n</code></pre> <p>This will create a mkdocs.yml file and a docs/ folder with an index.md file.</p> <pre><code>project/\n\u2502\n\u251c\u2500\u2500 src/calculator/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 calculations.py\n\u2502\n\u251c\u2500\u2500 docs/\n\u2502   \u2514\u2500\u2500 index.md\n\u2502\n\u251c\u2500\u2500 mkdocs.yml\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 pyproject.toml\n\u2514\u2500\u2500 ...\n</code></pre>"},{"location":"guides/mkdocs/#create-mkdocs-configuration-file","title":"Create MkDocs Configuration File","text":"<p>Create\u00a0mkdocs\u00a0Configuration File: Edit the\u00a0mkdocs.yml\u00a0file in the root of your project. This file will contain the configuration for your\u00a0mkdocs\u00a0site.</p> <pre><code>site_name: UV Data Science Project Template\nnav:\n  - Home: index.md\n  - Guides:\n    - Ruff: guides/ruff.md\n    - UV: guides/uv.md\n    - PyTest: guides/pytest.md\n    - PyRight: guides/pyright.md\n    - Pre-Commit: guides/pre-commit.md\n    - MkDocs: guides/mkdocs.md\n    - Docker-Production: guides/docker_prod.md\n    - Docker-VSCode-DevContainer: guides/docker_vscode_devcontainer.md\n    - CI-GitHub: guides/ci_github.md\n  - Source Code API Reference:\n      - Autoencoder: api/autoencoder.md\n      - Training: api/training.md\n      - FastAPI: api/fastapi_app.md\ntheme:\n  name: material\nplugins:\n  - search\n  - include-markdown\n  - mkdocstrings:\n      handlers:\n        python:\n          options:\n            docstring_style: google\n</code></pre>"},{"location":"guides/mkdocs/#generate-general-documentation","title":"Generate General Documentation","text":"<p>Generate General Documentation: For example use the <code>index.md</code> to reference your <code>README.md</code> file. <code>docs/index.md</code>:</p> <pre><code># Dev Container UV Data Science\n\nWelcome to the documentation for the Dev Container UV Data Science project.\n\n&lt;!-- Include the content of README.devcontainer.md --&gt;\n&lt;!-- Using PyMdown Snippets for inclusion relative to the base_path defined in the mkdocs.yml--&gt;\n&lt;!-- Uncomment the following lines to include the README.devcontainer.md file --&gt;\n&lt;!-- --8&lt;-- \".devcontainer/README.devcontainer.md\" --&gt;\n</code></pre>"},{"location":"guides/mkdocs/#generate-api-documentation","title":"Generate API Documentation","text":"<p>Use\u00a0<code>mkdocstrings</code>\u00a0to generate API documentation from your Python docstrings. In your\u00a0<code>autoencoder.md</code>\u00a0and\u00a0<code>training.md</code>\u00a0files, you can include references to your Python modules:</p> <ul> <li> <p>Create Documentation Files: Create a\u00a0docs\u00a0directory in the root of your project. Inside this directory, create the necessary Markdown files for your documentation.</p> <pre><code>mkdir docs/api\ntouch docs/api/autoencoder.md\ntouch docs/api/training.md\n</code></pre> </li> </ul> <p><code>docs/docs/autoencoder.md</code>:</p> <pre><code># Autoencoder\n\n::: dev_container_uv_datascience.lit_auto_encoder\n</code></pre> <p><code>docs/training.md</code>:</p> <pre><code># Training\n\n::: dev_container_uv_datascience.train_autoencoder\n</code></pre>"},{"location":"guides/mkdocs/#building-the-documentation","title":"Building the Documentation","text":"<p>To build the documentation, run the command below. This will generate a static site in the site directory.</p> <pre><code>uv run mkdocs build\n</code></pre>"},{"location":"guides/mkdocs/#serving-the-documentation","title":"Serving the Documentation","text":"<p>To serve the documentation locally, run the command below. This will start a local web server that you can use to view the documentation.</p> <pre><code>uv run mkdocs serve\n</code></pre>"},{"location":"guides/mkdocs/#deploying-the-documentation","title":"Deploying the Documentation","text":"<p>The documentation can be deployed to a variety of platforms, such as GitHub Pages, Netlify, or AWS S3. This project is set up to deploy to GitHub Pages using GitHub Actions. See the CI-GitHub guide for more information.</p> <p>To deploy your documentation, you can use the\u00a0<code>mkdocs gh-deploy</code>\u00a0command to deploy it to GitHub Pages:</p> <pre><code>uv run mkdocs gh-deploy\n</code></pre> <p>Ensure you enable GitHub pages via the repo settings, see:</p> <ul> <li>GitHub - Deploy MkDocs</li> <li>MkDocs - Deploying your docs</li> <li>Publishing your site</li> <li>GitHub Pages</li> </ul>"},{"location":"guides/pre_commit/","title":"Using Pre-Commit Hooks","text":"<p>Pre-commit hooks are scripts that run automatically before each commit. They help to identify and fix issues before they are integrated into the codebase, ensuring code quality and consistency.</p> <ul> <li>Using Pre-Commit Hooks</li> <li>Benefits of Using Pre-Commit Hooks?</li> <li>How to Use Pre-Commit Hooks in This Project<ul> <li>1. Installation</li> <li>2. Install Pre-Commit Hooks and/or Running Hooks Manually</li> <li>3. Configuring Hooks</li> <li>4. Updating hooks automatically</li> <li>5. Git Commit ignoring pre-commit hooks</li> <li>6. Customizing Hooks</li> </ul> </li> </ul>"},{"location":"guides/pre_commit/#benefits-of-using-pre-commit-hooks","title":"Benefits of Using Pre-Commit Hooks?","text":"<p>Pre-commit hooks automate code checks, formatting, and other tasks, saving time and effort during code reviews. They catch common problems early, preventing them from becoming bigger issues later.</p>"},{"location":"guides/pre_commit/#how-to-use-pre-commit-hooks-in-this-project","title":"How to Use Pre-Commit Hooks in This Project","text":"<p>This project uses pre-commit to manage pre-commit hooks. Here's how to get started:</p>"},{"location":"guides/pre_commit/#1-installation","title":"1. Installation","text":"<p>Make sure you have <code>uv</code> installed. If not, refer to the installation guide.</p>"},{"location":"guides/pre_commit/#2-install-pre-commit-hooks-andor-running-hooks-manually","title":"2. Install Pre-Commit Hooks and/or Running Hooks Manually","text":"<ul> <li> <p>Install Pre-Commit Hooks to automatically when you commit changes. Run the following command to install the pre-commit hooks:</p> <pre><code>uv run pre-commit install\n</code></pre> </li> <li> <p>Running Hooks Manually If you want to run the hooks manually, use the following command:</p> <pre><code>uv run pre-commit run --all-files\n</code></pre> </li> </ul>"},{"location":"guides/pre_commit/#3-configuring-hooks","title":"3. Configuring Hooks","text":"<p>The pre-commit hooks are configured in the <code>.pre-commit-config.yaml</code> file. This file specifies the hooks to use, their settings, and other options.</p> <p>Here's a breakdown of the hooks used in this project:</p> <ul> <li>Ruff: A fast Python linter and formatter. It checks for code style issues and automatically fixes them. Refer to the <code>ruff.toml</code> file for the configurations.<ul> <li>Ruff Check: Runs the Ruff linter to identify issues. The <code>--fix</code> argument automatically fixes fixable issues. <code>--exit-non-zero-after-fix</code> will cause the hook to fail if ruff makes changes.</li> <li>Ruff Format: Runs the Ruff formatter to format the code. The <code>--diff</code> argument displays a diff of the changes instead of applying them directly.</li> </ul> </li> <li>Pyright: A static type checker for Python. It checks for type errors in the code. Refer to the <code>pyrightconfig.json</code> file for the configurations.</li> <li>Hadolint: A linter for Dockerfiles. It checks for common issues and best practices in Dockerfiles. The <code>--config .hadolint.yaml</code> argument specifies a configuration file for Hadolint.</li> <li>check-added-large-files: Prevents committing large files to the repository.</li> <li>check-toml: Checks TOML files for syntax errors.</li> <li>check-yaml: Checks YAML files for syntax errors.</li> <li>end-of-file-fixer: Ensures that files end with a newline, and removes them if they don't.</li> <li>trailing-whitespace: Trims trailing whitespace from lines.</li> <li>TruffleHog: Detects secrets and credentials in your code.</li> </ul>"},{"location":"guides/pre_commit/#4-updating-hooks-automatically","title":"4. Updating hooks automatically","text":"<p>You can update your hooks to the latest version automatically by running <code>pre-commit autoupdate</code>. By default, this will bring the hooks to the latest tag on the default branch.</p>"},{"location":"guides/pre_commit/#5-git-commit-ignoring-pre-commit-hooks","title":"5. Git Commit ignoring pre-commit hooks","text":"<p>If you want to commit changes without running the pre-commit hooks, you can use the <code>--no-verify</code> flag with the <code>git commit</code> command.</p> <pre><code>git commit --no-verify -m \"Your commit message\"\n</code></pre>"},{"location":"guides/pre_commit/#6-customizing-hooks","title":"6. Customizing Hooks","text":"<p>You can customize the pre-commit hooks by modifying the <code>.pre-commit-config.yaml</code> file. You can add new hooks, remove existing hooks, or change the settings of existing hooks.</p> <p>For more information on how to configure pre-commit hooks, see the pre-commit documentation.</p>"},{"location":"guides/pyright/","title":"Pyright Usage Guide","text":"<p>Pyright is a static type checker for Python. It helps you catch type-related errors early in the development process, improving the reliability and maintainability of your code. This guide explains how to use Pyright in this project, including configuration and best practices.</p> <ul> <li>Pyright Usage Guide</li> <li>Installation</li> <li>Configuration</li> <li>Usage<ul> <li>VS Code Integration</li> <li>Command-Line Usage</li> <li>Ignoring Errors</li> </ul> </li> <li>Integration with Ruff</li> <li>Best Practices</li> </ul>"},{"location":"guides/pyright/#installation","title":"Installation","text":"<p>Pyright is included as a development dependency in this project. You can install it using <code>uv</code>:</p> <pre><code>uv add pyright\n</code></pre>"},{"location":"guides/pyright/#configuration","title":"Configuration","text":"<p>Pyright is configured using the <code>pyrightconfig.json</code> file in the root of the project. Here's a breakdown of the configuration options:</p> <ul> <li><code>pythonVersion</code>: Specifies the Python version used for type checking.</li> <li><code>pythonPlatform</code>: Specifies the target platform.</li> <li><code>venvPath</code>: Specifies the path to the virtual environment.</li> <li><code>typeCheckingMode</code>: Specifies the type checking mode. Options include <code>\"basic\"</code>, <code>\"strict\"</code>, and <code>\"standard\"</code>.</li> <li><code>include</code>: A list of directories to include in type checking.</li> <li><code>exclude</code>: A list of directories to exclude from type checking.</li> </ul> <pre><code>{\n    \"pythonVersion\": \"3.12\",\n    \"pythonPlatform\": \"All\",\n    \"venvPath\": \"./.venv\",\n    \"typeCheckingMode\": \"standard\",\n    \"include\": [\n        \"src/\",\n        \"tests/\"\n    ],\n    \"exclude\": [\n        \"**/__pycache__\",\n        \".pytest_cache\",\n        \".ruff_cache\",\n        \".venv\"\n    ]\n}\n</code></pre>"},{"location":"guides/pyright/#usage","title":"Usage","text":"<p>To run Pyright, use the following command:</p> <pre><code>uv run pyright\n</code></pre> <p>This will analyze the code in the <code>src/</code> and <code>tests/</code> directories based on the configuration in <code>pyrightconfig.json</code>.</p>"},{"location":"guides/pyright/#vs-code-integration","title":"VS Code Integration","text":"<p>For VS Code, the Pylance extension provides excellent integration with Pyright. It offers real-time type checking and code completion as you type.</p> <p>To enable Pylance, install the extension and ensure that it is configured to use the project's Python environment.</p>"},{"location":"guides/pyright/#command-line-usage","title":"Command-Line Usage","text":"<p>You can also run Pyright from the command line using the <code>pyright</code> command. This is useful for CI/CD pipelines and pre-commit hooks.</p> <pre><code>uv run pyright\n</code></pre>"},{"location":"guides/pyright/#ignoring-errors","title":"Ignoring Errors","text":"<p>Sometimes, you may want to ignore specific Pyright errors. You can do this using the <code># pyright: ignore</code> comment.</p> <pre><code>x = 1  # pyright: ignore\n</code></pre> <p>You can also ignore specific error codes:</p> <pre><code>x = 1  # pyright: ignore[reportGeneralTypeIssues]\n</code></pre>"},{"location":"guides/pyright/#integration-with-ruff","title":"Integration with Ruff","text":"<p>This project uses Ruff for linting and formatting. Ruff can also run Pyright as part of its checks. To enable this, configure Ruff to include Pyright rules. However, in this project, we run pyright separately.</p>"},{"location":"guides/pyright/#best-practices","title":"Best Practices","text":"<ul> <li>Write type hints for all functions and variables.</li> <li>Use the <code>typing</code> module for advanced type hinting.</li> <li>Run Pyright regularly to catch errors early.</li> <li>Configure your editor to use Pylance for real-time type checking.</li> </ul> <p>By following these guidelines, you can ensure that your code is well-typed and maintainable.</p>"},{"location":"guides/pytest/","title":"Pytest and Coverage Guide","text":"<p>This guide explains how to use <code>pytest</code> for testing and <code>coverage.py</code> for measuring test coverage in this project.</p> <ul> <li>Pytest and Coverage Guide</li> <li>Introduction to Pytest<ul> <li>Key Features</li> </ul> </li> <li>Running Tests<ul> <li>Specific Tests</li> </ul> </li> <li>Introduction to Coverage.py<ul> <li>Coverage Key Features</li> </ul> </li> <li>Running Tests with Coverage</li> <li>Configuration<ul> <li>.coveragerc</li> <li>pytest.ini or pyproject.toml</li> </ul> </li> <li>Usage in This Project</li> <li>Example Workflow</li> </ul>"},{"location":"guides/pytest/#introduction-to-pytest","title":"Introduction to Pytest","text":"<p>pytest is a powerful and flexible testing framework for Python. It simplifies writing and running tests, and provides a rich set of features for test discovery, parametrization, fixtures, and more.</p>"},{"location":"guides/pytest/#key-features","title":"Key Features","text":"<ul> <li>Simple Syntax: Pytest uses a simple and intuitive syntax for writing tests.</li> <li>Test Discovery: It automatically discovers test files and test functions.</li> <li>Fixtures: Fixtures provide a way to set up and tear down test environments.</li> <li>Plugins: A rich ecosystem of plugins extends pytest's functionality.</li> </ul>"},{"location":"guides/pytest/#running-tests","title":"Running Tests","text":"<p>To run tests, use the following command in the project's root directory:</p> <pre><code>uv run pytest\n</code></pre> <p>This command will discover and run all files matching <code>test_*.py</code> or <code>*_test.py</code> in the project's directory and subdirectories.</p>"},{"location":"guides/pytest/#specific-tests","title":"Specific Tests","text":"<p>To run a specific test file or test function, you can specify its path:</p> <pre><code>uv run pytest tests/test_module.py\nuv run pytest tests/test_module.py::test_function\n</code></pre>"},{"location":"guides/pytest/#introduction-to-coveragepy","title":"Introduction to Coverage.py","text":"<p>coverage.py is a tool for measuring code coverage in Python. It monitors your program, notes which parts of the code have been executed, and then analyzes the source to identify code that could have been executed but was not.</p>"},{"location":"guides/pytest/#coverage-key-features","title":"Coverage Key Features","text":"<ul> <li>Line Coverage: Measures which lines of code were executed during testing.</li> <li>Branch Coverage: Measures which branches in the code were taken during testing.</li> <li>Integration with Pytest: Seamless integration with pytest for running tests and collecting coverage data.</li> <li>Reporting: Generates reports in various formats, including HTML, XML, and text.</li> </ul>"},{"location":"guides/pytest/#running-tests-with-coverage","title":"Running Tests with Coverage","text":"<p>To run tests with coverage, use the following command:</p> <pre><code>uv run pytest --cov\n</code></pre> <p>This command will run all tests and collect coverage data for the <code>src</code> directory. The <code>--cov</code> flag specifies the source directory to measure coverage for.</p>"},{"location":"guides/pytest/#configuration","title":"Configuration","text":""},{"location":"guides/pytest/#coveragerc","title":".coveragerc","text":"<p>The <code>.coveragerc</code> file configures how <code>coverage.py</code> collects and reports coverage data. Key settings include:</p> <ul> <li><code>data_file</code>: Specifies the location where coverage data is stored. In this project, it's set to <code>.test_reports/coverage/.coverage</code>.</li> <li><code>source</code>: Specifies the source directories to measure coverage for. In this project, it's set to <code>src</code>.</li> <li><code>branch</code>: Enables branch coverage.</li> <li><code>omit</code>: Specifies files to exclude from coverage reporting.</li> <li><code>report</code>: This will create an HTML report in the <code>.test_reports/coverage/html</code> directory. You can then open the <code>index.html</code> file in your browser to view the report.</li> <li>XML Report: In the CI Workflow a XML report will be created in the <code>.test_reports/coverage/coverage.xml</code> file, which can be used for integration with CI/CD systems.</li> </ul>"},{"location":"guides/pytest/#pytestini-or-pyprojecttoml","title":"pytest.ini or pyproject.toml","text":"<p>Pytest can be configured using a <code>pytest.ini</code> file or a <code>pyproject.toml</code> file. This project uses a <code>pytest.ini</code> file.</p> <ul> <li><code>testpaths</code>: Specifies the directories to search for tests.</li> <li><code>addopts</code>: Specifies command-line options to always use when running pytest.</li> <li><code>filterwarnings</code>: To handle warnings during testing.</li> </ul>"},{"location":"guides/pytest/#usage-in-this-project","title":"Usage in This Project","text":"<p>This project is set up to automatically collect coverage data when running tests. The <code>.coveragerc</code> file is configured to:</p> <ul> <li>Store coverage data in the <code>.test_reports/coverage/</code> directory.</li> <li>Measure coverage for the <code>src</code> directory.</li> <li>Generate Terminal reports in the <code>.test_reports/coverage/</code> directory.</li> <li>Generate HTML reports in the <code>.test_reports/coverage/html</code> directory.</li> </ul>"},{"location":"guides/pytest/#example-workflow","title":"Example Workflow","text":"<ol> <li>Write your code in the <code>src</code> directory.</li> <li>Write tests for your code in the <code>tests</code> directory.</li> <li>Run tests with coverage: <code>uv run pytest --cov</code></li> <li>Check the terminal report to see which parts of your code are not covered by tests.</li> <li>Write more tests to increase coverage.</li> <li>Repeat steps 3-5 until you have sufficient coverage.</li> </ol> <p>By following this guide, you can effectively use <code>pytest</code> and <code>coverage.py</code> to write and test your code, ensuring high code quality and reliability.</p>"},{"location":"guides/ruff/","title":"Ruff","text":"<p>Ruff is an extremely fast Python linter, formatter, and code assistant. It's written in Rust and is designed to be a drop-in replacement for tools like Flake8, pycodestyle, pyupgrade, isort, and others.</p> <ul> <li>Ruff</li> <li>Key Features</li> <li>Ruff in this Project<ul> <li>Configuration (<code>ruff.toml</code>)</li> <li>Usage</li> <li>Pre-commit Hooks</li> <li>Command Line</li> <li>Integrating with Editors</li> <li>Customizing Ruff</li> <li>Pydocstyle Convention</li> </ul> </li> </ul>"},{"location":"guides/ruff/#key-features","title":"Key Features","text":"<ul> <li>Extremely Fast: Written in Rust, Ruff is significantly faster than traditional Python linters.</li> <li>Comprehensive: Combines the functionality of multiple tools, including linters, formatters, and code fixers.</li> <li>Configurable: Highly customizable through the <code>ruff.toml</code> configuration file.</li> <li>Easy to Integrate: Integrates well with popular editors and CI/CD systems.</li> <li>Automatic Fixes: Can automatically fix many common linting errors.</li> </ul>"},{"location":"guides/ruff/#ruff-in-this-project","title":"Ruff in this Project","text":"<p>This project uses Ruff for linting, formatting, and ensuring code quality. The configuration is managed through the <code>ruff.toml</code> file located in the root directory of the project.</p>"},{"location":"guides/ruff/#configuration-rufftoml","title":"Configuration (<code>ruff.toml</code>)","text":"<p>The <code>ruff.toml</code> file specifies the settings for Ruff. Here's a breakdown of the key configurations:</p> <ul> <li><code>exclude</code>: Specifies directories that Ruff should ignore, such as <code>.git</code>, <code>.venv</code>, <code>dist</code>, etc.</li> <li><code>line-length</code>: Sets the maximum line length for the project (currently 99 characters).</li> <li><code>target-version</code>: Specifies the target Python version (currently Python 3.12).</li> <li><code>lint.select</code>: Defines the set of rules that Ruff should enforce. This includes:<ul> <li><code>E</code>: pycodestyle errors</li> <li><code>N</code>: pep8-naming conventions</li> <li><code>F</code>: pyflakes</li> <li><code>D</code>: pydocstyle</li> </ul> </li> <li><code>lint.ignore</code>: Specifies rules that should be ignored. This project ignores several pydocstyle rules related to missing docstrings in certain contexts.</li> <li><code>fixable</code>: Specifies rules that Ruff can automatically fix.</li> <li><code>dummy-variable-rgx</code>: Configures the regular expression for dummy variable names.</li> <li><code>format</code>: Configures code formatting options such as quote style, indentation, etc.</li> <li><code>per-file-ignores</code>: Allows specifying different configurations for different files or directories.  Tests directory ignores missing docstrings.</li> </ul>"},{"location":"guides/ruff/#usage","title":"Usage","text":"<p>Ruff is integrated into the development workflow using pre-commit hooks, the CI/CD pipelines and will be applied in auto save mode when using VS Code.</p>"},{"location":"guides/ruff/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>Ruff is configured as a pre-commit hook to automatically lint and format code before each commit. This helps ensure that all code meets the project's quality standards. To use the pre-commit hooks, make sure you have pre-commit installed and configured in your local environment.</p>"},{"location":"guides/ruff/#command-line","title":"Command Line","text":"<p>Ruff can also be run from the command line:</p> <pre><code>uv run ruff check .\nuv run ruff format .\n</code></pre> <p>These commands will check and format the code in the current directory, respectively.</p>"},{"location":"guides/ruff/#integrating-with-editors","title":"Integrating with Editors","text":"<p>Ruff integrates with many popular code editors, such as VS Code, Sublime Text, and others. Refer to the Ruff documentation for editor-specific instructions.</p>"},{"location":"guides/ruff/#customizing-ruff","title":"Customizing Ruff","text":"<p>The <code>ruff.toml</code> file can be customized to suit the specific needs of the project.  Refer to the Ruff documentation for a complete list of available options.</p>"},{"location":"guides/ruff/#pydocstyle-convention","title":"Pydocstyle Convention","text":"<p>The <code>pydocstyle</code> section specifies the convention for docstrings. This project uses the \"google\" convention.</p>"},{"location":"guides/uv/","title":"UV, an extremely fast Python package and project manager, written in Rust","text":"<p>UV is used as a fast and efficient package manager for this project, replacing tools like pip, virtualenv, and pip-tools. It significantly speeds up the installation of dependencies and management of the virtual environment, making the development process smoother and faster. UV ensures consistent dependency resolution and environment setup across different development environments.</p> <ul> <li>UV, an extremely fast Python package and project manager, written in Rust</li> <li>Getting Started with UV<ul> <li>Install uv</li> <li>macOS and Linux</li> <li>Windows</li> <li>Install Python</li> <li>Install Python packages and dependencies</li> <li>Run Python code</li> <li>Launching JupyterLab</li> <li>Optional: Manage virtual environments manually</li> <li>UV self update</li> <li>UV update packages (dependencies)</li> </ul> </li> </ul> <p>UV, an extremely fast Python package, virtual environment and project manager.</p> <ul> <li>\ud83d\ude80 A single tool to replace pip, pip-tools, pipx, poetry, pyenv, twine, virtualenv, and more.</li> <li>\u26a1\ufe0f 10-100x faster than pip.</li> <li>\ud83d\udc0d Installs and manages Python versions.</li> <li>\ud83d\udee0\ufe0f Runs and installs Python applications.</li> <li>\u2747\ufe0f Runs single-file scripts, with support for inline dependency metadata.</li> <li>\ud83d\uddc2\ufe0f Provides comprehensive project management, with a universal lockfile.</li> <li>\ud83d\udd29 Includes a pip-compatible interface for a performance boost with a familiar CLI.</li> <li>\ud83c\udfe2 Supports Cargo-style workspaces for scalable projects.</li> </ul> <p>Specifically, UV is utilized for:</p> <ul> <li>Dependency Management: UV manages both project dependencies and development dependencies, ensuring that all necessary packages are installed quickly and efficiently. This includes <code>pydantic-settings</code> for robust configuration management.</li> <li>Virtual Environment Creation: UV creates and manages the project's virtual environment, isolating project dependencies from the global Python environment.</li> <li>Speed and Efficiency: UV's speed advantage over traditional tools like <code>pip</code> significantly reduces the time spent on environment setup and dependency installation, especially in projects with many dependencies.</li> <li>Consistency: By using UV, the project ensures that the development environment is consistent across different machines and platforms, reducing the risk of \"it works on my machine\" issues.</li> <li>Integration with pyproject.toml: UV leverages the <code>pyproject.toml</code> file for project configuration, making it easy to define and manage dependencies, scripts, and other project settings.</li> <li>Running Tools: UV is used to run development tools like <code>ruff</code>, <code>pytest</code>, and <code>toml-sort</code> directly from the command line, ensuring that the correct versions of these tools are used and that they are isolated from other projects. Example: <code>uv run ruff .</code></li> <li>Packaging: UV can be used to package the project for distribution, creating a <code>wheel</code> file that can be uploaded to PyPI or installed directly.</li> <li>Lockfile Management: UV uses a <code>uv.lock</code> file to ensure that all dependencies are installed at the exact versions specified, preventing compatibility issues and ensuring reproducibility.</li> </ul> <p>Key features of UV include:</p> <ul> <li>Written in Rust: UV is written in Rust, which provides excellent performance and memory safety.</li> <li>Compatibility: UV is compatible with <code>pip</code> and <code>virtualenv</code>, so it can be used with existing Python projects without requiring major changes.</li> <li>pyproject.toml Support: UV fully supports the <code>pyproject.toml</code> file, which is the recommended way to configure Python projects.</li> <li>Speed: UV is significantly faster than <code>pip</code> for most operations, especially when installing dependencies from scratch.</li> <li>Global Cache: UV uses a global cache to store downloaded packages, so they don't need to be downloaded again for each project.</li> </ul>"},{"location":"guides/uv/#getting-started-with-uv","title":"Getting Started with UV","text":"<p>Refer to the official UV - Getting Started - Documentation for detailed instructions on installing and using UV.</p> <p>This section guides you through the Python setup and package installation procedure using <code>uv</code> native commands over the <code>uv pip</code> interface and is based on the content of Sebastian Raschka's GitHub repository.</p> <p>[!NOTE] There are alternative ways to install Python and use <code>uv</code>. For example, you can install Python directly via <code>uv</code> and use <code>uv add</code> instead of <code>uv pip install</code> for even faster package management.</p> <p>If you prefer the <code>uv pip</code> commands, I recommend checking the official <code>uv</code> documentation.</p> <p>While <code>uv add</code> offers additional speed advantages, <code>uv pip</code> might be slightly more user-friendly for with your existing habits. However, if you're new to Python package management, the native <code>uv</code> interface is also a great opportunity to learn it from the start. It's also how I use <code>uv</code>, but I realized the barrier to entry is a bit higher if you are coming from <code>pip</code> and <code>conda/mamba</code>.</p>"},{"location":"guides/uv/#install-uv","title":"Install uv","text":"<p>Uv can be installed as follows, depending on your operating system.</p>"},{"location":"guides/uv/#macos-and-linux","title":"macOS and Linux","text":"<pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> <p>or</p> <pre><code>wget -qO- https://astral.sh/uv/install.sh | sh\n</code></pre>"},{"location":"guides/uv/#windows","title":"Windows","text":"<pre><code>powershell -c \"irm https://astral.sh/uv/install.ps1 | more\"\n</code></pre>"},{"location":"guides/uv/#install-python","title":"Install Python","text":"<p>You can install Python using uv:</p> <pre><code>uv python install 3.10\n</code></pre>"},{"location":"guides/uv/#install-python-packages-and-dependencies","title":"Install Python packages and dependencies","text":"<p>To install all required packages from a <code>pyproject.toml</code> file (such as the one located at the top level of this GitHub repository), run the following command, assuming the file is in the same directory as your terminal session:</p> <pre><code>uv add . --group dev\n</code></pre> <p>[!NOTE] If you have problems with the following commands above due to certain dependencies (for example, if you are using Windows), you can always fall back to regular pip: <code>uv add pip</code> <code>uv run python -m pip install -U -r requirements.txt</code></p> <p>Note that the <code>uv add</code> command above will create a separate virtual environment via the <code>.venv</code> subfolder. (In case you want to delete your virtual environment to start from scratch, you can simply delete the <code>.venv</code> folder.)</p> <p>You can install new packages, that are not specified in the <code>pyproject.toml</code> via <code>uv add</code>, for example:</p> <pre><code>uv add packaging\n</code></pre> <p>And you can remove packages via <code>uv remove</code>, for example:</p> <pre><code>uv remove packaging\n</code></pre>"},{"location":"guides/uv/#run-python-code","title":"Run Python code","text":"<p>Your environment should now be ready to run the code in the repository. You can test it by running the following command to run a python script:</p> <pre><code>uv run python main.py\n</code></pre> <p>Or, if you don't want to type <code>uv run python</code> every time you execute code, manually activate the virtual environment first.</p> <p>On macOS/Linux:</p> <pre><code>source .venv/bin/activate\n</code></pre> <p>On Windows (PowerShell):</p> <pre><code>.venv\\Scripts\\activate\n</code></pre> <p>Then, run:</p> <pre><code>python main.py\n</code></pre>"},{"location":"guides/uv/#launching-jupyterlab","title":"Launching JupyterLab","text":"<p>You can launch a JupyterLab instance via:</p> <pre><code>uv run jupyter lab\n</code></pre>"},{"location":"guides/uv/#optional-manage-virtual-environments-manually","title":"Optional: Manage virtual environments manually","text":"<p>Alternatively, you can still install the dependencies directly from the repository using <code>uv pip install</code>. But note that this doesn't record dependencies in a <code>uv.lock</code> file as <code>uv add</code> does. Also, it requires creating and activating the virtual environment manually:</p> <ol> <li>Create a new virtual environment</li> </ol> <p>Run the following command to manually create a new virtual environment, which will be saved via a new <code>.venv</code> subfolder:</p> <pre><code>uv venv --python=python3.12\n</code></pre> <ol> <li>Activate virtual environment</li> </ol> <p>Next, we need to activate this new virtual environment.</p> <p>On macOS/Linux:</p> <pre><code>source .venv/bin/activate\n</code></pre> <p>On Windows (PowerShell):</p> <pre><code>.venv\\Scripts\\activate\n</code></pre> <ol> <li>Install dependencies</li> </ol> <p>To install the required dependencies from your local <code>requirements.txt</code> file, use the following command:</p> <pre><code>uv pip install -r requirements.txt\n</code></pre>"},{"location":"guides/uv/#uv-self-update","title":"UV self update","text":"<pre><code>uv self update\n</code></pre>"},{"location":"guides/uv/#uv-update-packages-dependencies","title":"UV update packages (dependencies)","text":"<p>You can update all Python packages to the latest versions of the <code>uv.lock</code> file and sync the environment using the command below. This will update all packages to their latest versions, which may not always be desirable. If you want to update only specific packages, you can use <code>uv lock --upgrade &lt;package_name&gt;</code> instead. This also does not update the specified dependencies within the <code>pyproject.toml</code> file. As of uv version 0.7.22 there is no way to update the dependencies in the <code>pyproject.toml</code> file automatically. Draft pull request: #13934. You can update individual packages manually to the latest compatible version within the <code>pyproject.toml</code> file by using <code>uv add --upgrade &lt;package_name&gt;</code>.</p> <pre><code>uv lock --upgrade\nuv sync --all-groups\n</code></pre> <p>[!NOTE] Upgrade dependencies within the <code>pyproject.toml</code> file with the script <code>scripts/uv_upgrade_dependencies.py</code> respectively the Makefile (project root directory) target <code>update-packages</code>, source:</p> <pre><code>make update-packages\n</code></pre>"},{"location":"notebooks/app_without_fastapi/","title":"Jupyter Notebook - App","text":"In\u00a0[\u00a0]: Copied! <pre>import warnings\nfrom typing import List, Tuple\n\nfrom pydantic import BaseModel, Field\nfrom torch import rand\nfrom torch.nn import Sequential\nfrom typing_extensions import Annotated\n\nfrom uv_datascience_project_template.lit_auto_encoder import LitAutoEncoder\nfrom uv_datascience_project_template.train_autoencoder import train_litautoencoder\n\nwarnings.filterwarnings(\"ignore\")\n</pre> import warnings from typing import List, Tuple  from pydantic import BaseModel, Field from torch import rand from torch.nn import Sequential from typing_extensions import Annotated  from uv_datascience_project_template.lit_auto_encoder import LitAutoEncoder from uv_datascience_project_template.train_autoencoder import train_litautoencoder  warnings.filterwarnings(\"ignore\") In\u00a0[\u00a0]: Copied! <pre># Input validation model\nclass NumberFakeImages(BaseModel):\n    n_fake_images: Annotated[int, Field(ge=1, le=10)]  # type: ignore # Between 1 and 10 fake images allowed\n</pre> # Input validation model class NumberFakeImages(BaseModel):     n_fake_images: Annotated[int, Field(ge=1, le=10)]  # type: ignore # Between 1 and 10 fake images allowed In\u00a0[\u00a0]: Copied! <pre>def train_model() -&gt; Tuple[Sequential, Sequential]:\n    \"\"\"Train the autoencoder model.\n\n    Returns:\n        tuple[Sequential, Sequential]: Encoder and decoder models.\n    \"\"\"\n\n    encoder, decoder, _is_model_trained = train_litautoencoder()\n    return encoder, decoder\n</pre> def train_model() -&gt; Tuple[Sequential, Sequential]:     \"\"\"Train the autoencoder model.      Returns:         tuple[Sequential, Sequential]: Encoder and decoder models.     \"\"\"      encoder, decoder, _is_model_trained = train_litautoencoder()     return encoder, decoder In\u00a0[\u00a0]: Copied! <pre># Train encoder and decoder\nencoder, decoder = train_model()\n</pre> # Train encoder and decoder encoder, decoder = train_model() <pre>GPU available: True (mps), used: True\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\n\n  | Name    | Type       | Params | Mode \n-----------------------------------------------\n0 | encoder | Sequential | 50.4 K | train\n1 | decoder | Sequential | 51.2 K | train\n-----------------------------------------------\n101 K     Trainable params\n0         Non-trainable params\n101 K     Total params\n0.407     Total estimated model params size (MB)\n8         Modules in train mode\n0         Modules in eval mode\n</pre> <pre>Epoch 0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:00&lt;00:00, 203.43it/s, v_num=80]</pre> <pre>`Trainer.fit` stopped: `max_epochs=1` reached.\n</pre> <pre>Epoch 0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:00&lt;00:00, 198.91it/s, v_num=80]\n</pre> In\u00a0[\u00a0]: Copied! <pre>def create_embed(\n    input_data: NumberFakeImages, encoder: Sequential, decoder: Sequential, checkpoint_path: str\n) -&gt; List[List[float]]:\n    \"\"\"Embed fake images using the trained autoencoder.\n\n    Args:\n        input_data (NumberFakeImages): Input data containing the number of fake images to embed.\n        encoder (Sequential): Encoder model.\n        decoder (Sequential): Decoder model.\n        checkpoint_path (str): Path to the checkpoint file.\n\n    Returns:\n        List[List[float]]: A list containing the embeddings of each fake images as a list.\n    \"\"\"\n\n    n_fake_images = input_data.n_fake_images\n\n    # Load the trained autoencoder from the checkpoint\n    autoencoder = LitAutoEncoder.load_from_checkpoint(\n        checkpoint_path, encoder=encoder, decoder=decoder\n    )\n    encoder_model = autoencoder.encoder\n    encoder_model.eval()\n\n    # Generate fake image embeddings based on user input\n    fake_image_batch = rand(n_fake_images, 28 * 28, device=autoencoder.device)\n    embeddings = encoder_model(fake_image_batch)\n\n    return embeddings.tolist()\n</pre> def create_embed(     input_data: NumberFakeImages, encoder: Sequential, decoder: Sequential, checkpoint_path: str ) -&gt; List[List[float]]:     \"\"\"Embed fake images using the trained autoencoder.      Args:         input_data (NumberFakeImages): Input data containing the number of fake images to embed.         encoder (Sequential): Encoder model.         decoder (Sequential): Decoder model.         checkpoint_path (str): Path to the checkpoint file.      Returns:         List[List[float]]: A list containing the embeddings of each fake images as a list.     \"\"\"      n_fake_images = input_data.n_fake_images      # Load the trained autoencoder from the checkpoint     autoencoder = LitAutoEncoder.load_from_checkpoint(         checkpoint_path, encoder=encoder, decoder=decoder     )     encoder_model = autoencoder.encoder     encoder_model.eval()      # Generate fake image embeddings based on user input     fake_image_batch = rand(n_fake_images, 28 * 28, device=autoencoder.device)     embeddings = encoder_model(fake_image_batch)      return embeddings.tolist() In\u00a0[\u00a0]: Copied! <pre># Create embeddings\nembeddings = create_embed(\n    NumberFakeImages(n_fake_images=2),\n    encoder,\n    decoder,\n    checkpoint_path=\"./lightning_logs/LitAutoEncoder/version_0/checkpoints/epoch=0-step=100.ckpt\",\n)\n\n# Print the embeddings\nprint(\"\u26a1\" * 20, \"\\nPredictions (image embeddings):\\n\", embeddings, \"\\n\", \"\u26a1\" * 20, sep=\"\")\n</pre> # Create embeddings embeddings = create_embed(     NumberFakeImages(n_fake_images=2),     encoder,     decoder,     checkpoint_path=\"./lightning_logs/LitAutoEncoder/version_0/checkpoints/epoch=0-step=100.ckpt\", )  # Print the embeddings print(\"\u26a1\" * 20, \"\\nPredictions (image embeddings):\\n\", embeddings, \"\\n\", \"\u26a1\" * 20, sep=\"\") <pre>\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\nPredictions (image embeddings):\n[[0.3581562042236328, 0.14595450460910797, 0.47827404737472534], [0.5079143643379211, 0.09660176187753677, 0.5766311287879944]]\n\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\u26a1\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/app_without_fastapi/#jupyter-notebook-app","title":"Jupyter Notebook - App\u00b6","text":"<p>This notebook demonstrates how to use the LitAutoEncoder model interactively, without deploying a FastAPI server. You can train the model and generate embeddings for fake images directly in this notebook. This is useful for experimentation, prototyping, and understanding the workflow before production deployment.</p>"},{"location":"notebooks/app_without_fastapi/#run-application-interactively-no-fastapi","title":"Run Application Interactively (No FastAPI)\u00b6","text":"<p>Use this notebook to train and test the autoencoder model in a Jupyter environment. All steps are self-contained.</p>"},{"location":"notebooks/app_without_fastapi/#train-the-model","title":"Train the Model\u00b6","text":"<p>This step initializes and trains the autoencoder on the MNIST dataset. The encoder and decoder will be returned for further use.</p>"},{"location":"notebooks/app_without_fastapi/#create-embeddings","title":"Create Embeddings\u00b6","text":"<p>Generate embeddings for a batch of fake images using the trained encoder. The output is a list of latent vectors representing the input images.</p>"}]}